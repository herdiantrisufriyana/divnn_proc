---
title: 'Deep-insight visible neural network (DI-VNN) for improving 
        interpretability of a non-image deep learning model by data-driven 
        ontology'
# author:
#   - name: Herdiantri Sufriyana
#     affiliation:
#     - &gibi Graduate Institute of Biomedical Informatics, College of Medical
#       Science and Technology, Taipei Medical University, Taipei, Taiwan
#     - Department of Medical Physiology, College of Medicine, University of
#       Nahdlatul Ulama Surabaya, Surabaya, Indonesia
#     email: herdiantrisufriyana@unusa.ac.id
#   - name: Yu-Wei Wu
#     affiliation:
#     - *gibi
#     - &tmuh Clinical Big Data Research Center, Taipei Medical University
#       Hospital, Taipei, Taiwan
#   - name: Emily Chia-Yu Su
#     affiliation:
#     - *gibi
#     - *tmuh
#     - Research Center for Artificial Intelligence in Medicine, Taipei Medical
#       University, Taipei, Taiwan
output: 
  # pdf_document
  word_document:
    reference_docx: styles_divnn_proc.docx
always_allow_html: yes
---

\tableofcontents
\newpage

# Introduction

In this Supplementary Information, we describe details on this study following 
chronological order of the deep-insight visible neural network (DI-VNN) 
pipeline. We would use prelabor rupture of membranes (PROM) as an example. 
There are three of six sections corresponding to some sections in the 
main text, which are respectively Introduction, Software and equipment, and 
Procedure. Along with this PDF document, we also provide R Markdown (.Rmd) 
containing the same texts with this document but including the programming 
codes for the data analysis in-between of these texts. The R Markdown are 
available in https://github.com/herdiantrisufriyana/divnn_proc. To get raw 
data, one need to request an access from the BPJS Kesehatan for their sample 
dataset published in August 2019. Up to this date, there are three sample 
datasets they published in February 2019, August 2019, and December 2020. For 
the first and second versions, a request is applied via 
https://e-ppid.bpjs-kesehatan.go.id/, while the third is applied via 
https://data.bpjs-kesehatan.go.id. To preprocess the raw data into the input 
dataset of this study, follow the codes of the R Markdown in 
https://github.com/herdiantrisufriyana/medhist/tree/main/preprocessing.

# Software and equipment

We set up a programming environment for this study. Bioconductor was utilized 
as described in the main text. There were 179 R packages which are 10 base 
packages, 48 other packages, and 121 dependencies.

```{r Set up reproducible environment, include=FALSE}
if(!require(renv)) install.packages('renv')
if(!file.exists('renv')) renv::init(restart=F)
```

```{r Set sample kind, include=FALSE}
# sample.kind=NULL # if using R 3.5 or earlier
sample.kind='Rounding' # if using R 3.6 or later
```

```{r Set to run or not run very heavy computations, include=FALSE}
# Many computations were very heavy;
# thus, we provided the RDS files as substitutes and load only ones
# that can be ran in most computers.
# Set to TRUE if you want to run the very heavy computations.
run_heavy_computation=F
```

```{r Recommended memory limit, include=FALSE}
# Minimum 1 GB free memory should be allocated
memory.limit(size=1000000)
```

```{r Files not included in GitHub for some reasons, eval=FALSE, include=FALSE}
# Explicitly including data source
# Send a data access request to the BPJS Kesehatan. Use their data and our 
# preprocessing codes to get these files.

# data/public2.rds
# data/target_population.rds
# data/pregnancy_status.rds
# data/outcome.rds
# data/target_visits.rds
# data/annotation.rds
# data/mh_nationwide.rds
# data/mh_provider.rds
# data/cf_nationwide.rds
# data/cf_provider.rds
# data/inferdata.rds
# data/predidata.rds
# data/infercause.rds
# data/predicause.rds
# data/int_nps.rds

# Too large for GitHub (>25 mb)
# If one needs these files below, send email to herdiantrisufriyana@unusa.ac.id.

# data/pw_hlin.rds
# data/nw_int_hlin.rds
# data/test_data_reg.rds
# data/eval_divnn.rds
# data/calib_divnn.rds
```

```{r Install and set specific version of Bioconductor, include=FALSE}
source('R/check_install_load-function.R')

# Install devtools to install specific version of BiocManager.
check_install_load('devtools',version='2.4.1',repo='cran',load=F)

# Install specific version of BiocManager and Bioconductor.
check_install_load('BiocManager',version='1.30.10',repo='cran',load=F)
install_steps=T
if(BiocManager::version()!='3.11'){
  BiocManager::install(version='3.11',update=TRUE,ask=FALSE)
  install_steps=c(F,T)
}
```

```{r Install and load packages with specific version, include=FALSE}
for(i in install_steps){
  check_install_load('tidyverse','1.3.0',repo='bioc',load=i)
  check_install_load('dslabs','0.7.3',repo='bioc',load=i)
  check_install_load('kableExtra','1.3.1',repo='bioc',load=i)
  check_install_load('parallel','4.0.2',repo='bioc',load=i)
  check_install_load('doParallel','1.0.16',repo='bioc',load=i)
  check_install_load('pbapply','1.4.3',repo='bioc',load=i)
  check_install_load('lubridate','1.7.9',repo='bioc',load=i)
  check_install_load('broom','0.7.3',repo='bioc',load=i)
  check_install_load('caret','6.0.86',repo='bioc',load=i)
  check_install_load('igraph','1.2.6',repo='bioc',load=i)
  check_install_load('gam','1.20',repo='bioc',load=i)
  check_install_load('preprocessCore','1.50.0',repo='bioc',load=i)
  check_install_load('limma','3.44.3',repo='bioc',load=i)
  check_install_load('AnnotationDbi','1.50.3',repo='bioc',load=i)
  check_install_load('Rcpp','1.0.7',repo='bioc',load=i)
  check_install_load('GO.db','3.11.4',repo='bioc',load=i)
  check_install_load('WGCNA','1.69',repo='bioc',load=i)
  check_install_load('matrixStats','0.57.0',repo='bioc',load=i)
  check_install_load('Rtsne','0.15',repo='bioc',load=i)
  check_install_load('reticulate','1.16',repo='bioc',load=i)
  check_install_load('Biobase','2.48.0',repo='bioc',load=i)
  check_install_load('medhist','0.1.0',repo='herdiantrisufriyana',load=i)
  check_install_load('zeallot','0.1.0',repo='bioc',load=i)
  check_install_load('divnn','0.1.3',repo='herdiantrisufriyana',load=i)
  check_install_load('clixo','0.1.1',repo='herdiantrisufriyana',load=i)
  check_install_load('MLeval','0.3',repo='bioc',load=i)
  check_install_load('ggnetwork','0.5.8',repo='bioc',load=i)
  check_install_load('visNetwork','2.0.9',repo='bioc',load=i)
  check_install_load('ggpubr','0.4.0',repo='bioc',load=i)
  check_install_load('extrafont','0.17',repo='bioc',load=i)
  check_install_load('ggsci','2.9',repo='bioc',load=i)
  check_install_load('readxl','1.3.1',repo='bioc',load=i)
  check_install_load('tensorflow','2.0.0',repo='devtools',load=F)
  check_install_load('keras','2.3.0.0',repo='devtools',load=F)
  
  if(i){
    options(dplyr.summarise.inform=F)
    dslabs::ds_theme_set()
    select=dplyr::select
    rename=dplyr::rename
    slice=dplyr::slice
    use_condaenv('./renv/python/condaenvs/renv-python',required=T)
    renv::use_python(name='./renv/python/condaenvs/renv-python')
    renv::restore()
  }else{
    extrafont::font_import()
    reticulate::conda_create(
      envname='./renv/python/condaenvs/renv-python'
      ,packages='python=3.6.3'
    )
    reticulate::use_condaenv('./renv/python/condaenvs/renv-python',required=T)
    renv::use_python(name='./renv/python/condaenvs/renv-python')
    keras::install_keras(
      method='conda',
      version='2.3.0',
      tensorflow='2.0.0-gpu',
      envname='./renv/python/condaenvs/renv-python',
      conda_python_version='3.6.3',
      extra_packages=
        c('numpy','pandas','matplotlib==3.1.0','scikit-learn','h5py==2.10.0'),
      restart_session=F
    )
    # Please use console for this python installation.
    reticulate::py_install(
      envname='./renv/python/condaenvs/renv-python'
      ,packages='h5py==2.10.0'
      ,python_version='3.6.3'
      ,pip=T
      ,pip_options='--force-reinstall'
      ,pip_ignore_installed=T
    )
    renv::snapshot()
  }
}

rm(i)
```

```{r Load log for the expensive computation, include=FALSE}
# This is needed to print messages at the time we run the expensive computation.
source('data/log.R')
```

```{r Save package tables and the versions, eval=FALSE, include=FALSE}
rbind(
  
    # The base packages
    sessionInfo()$basePkgs %>%
      data.frame(
        package=.
        ,version=
          paste0(
            sessionInfo()$R.version$major
            ,'.'
            ,sessionInfo()$R.version$minor
          )
        ,base='Yes'
        ,loaded='Yes'
        ,attached='Yes'
      )
    
    # Additional packages specific to this study
    ,sessionInfo()$otherPkgs %>%
      lapply(function(x)data.frame(version=x$Version)) %>%
      do.call(rbind,.) %>%
      rownames_to_column(var='package') %>%
      mutate(base='No',loaded='Yes',attached='Yes')
    
    # The explicit dependencies
    ,sessionInfo()$loadedOnly %>%
      lapply(function(x)data.frame(version=x$Version)) %>%
      do.call(rbind,.) %>%
      rownames_to_column(var='package') %>%
      mutate(base='No',loaded='Yes',attached='No')
    
  ) %>%
  setNames(str_to_sentence(colnames(.))) %>%
  # group_by(Base,Loaded,Attached) %>%
  # summarize(n=n())
  
  kable() %>%
  kable_classic()
```

# Procedure

## Step 1

The data source was a sample dataset of the whole health insurance database 
during 2015 and 2016 by cross-sectional design. Stratified random sampling was 
applied. The strata variable was constructed from 66,072 combinations of all 
the healthcare facilities (*n*=22,024) and category of family, which were: (1) 
a family of which members never visit the healthcare facilities; (2) a family 
of which members have visited only primary care; and (3) a family of which 
members have visited all levels of care. For each stratum, one to ten families 
were randomly included. This means only 10 families were randomly included if 
more than that number, resulting 586,969 families with 1,697,452 subjects.

We conducted non-essential data cleaning, e.g. revising the inconsistent name 
of states, estimating the healthcare identifiers, *et cetera*. These procedures 
were parts of our R package of medhist 0.1.0. No sampling was conducted.

```{r Read public data II tables from an R file, eval=FALSE, include=FALSE}
# This is the output of the raw data after being preprocessed by data.Rmd in:
# https://github.com/herdiantrisufriyana/medhist/preprocessing
public=readRDS('data/public2.rds')
```

```{r Combine tables, eval=FALSE, include=FALSE}
public$visits=
  
  # Access a list of visit datasets based on capitation, fee for service (FFS), 
  # and diagnosis-related group (DRG).
  public[c('visit_cap','visit_ffs','visit_drg')] %>%
  
  # For each element of the list, select 4 columns and bind the elements by row.
  lapply(select,visit_id,subject_id,healthcare_id,admission_date) %>%
  do.call(rbind,.) %>%
  
  # Join with diagnosis dataset by visit_id, follow the dataframe above.
  left_join(public$diagnosis,by='visit_id') %>%
  
  # Exclude diagnoses at admission since it's unlikely definitive.
  filter(!code_type%in%c('Admission diagnosis')) %>%
  select(-code_type) %>%
  
  # Find the earliest visit per subject and filter only the unique visits.
  group_by(subject_id) %>%
  mutate(db_start_date=min(admission_date)) %>%
  ungroup() %>%
  filter(!duplicated(.))
```

After the non-essential data cleaning, we applied retrospective cohort design, 
as described in the main text. For pregnant women, we use several codes for 
determining delivery or immediately after delivery care. The 220 codes are 
described.

```{r Determine target population, eval=FALSE, include=FALSE}
public$target_population=
  
  ##### The health insurance holders of #####
  public$subject %>%
  lapply(X=1,Y=.,function(X,Y){
    # Record this step to the table of subject selection
    data.frame(
        step='total'
        ,exc_visit=0
        ,inc_visit=sum(public$visits$subject_id %in% Y$subject_id)
        ,exc_subject=0
        ,inc_subject=nrow(Y)
      ) %>%
      saveRDS('data/selection.rds')
    Y
  }) %>%
  .[[1]] %>%

  ##### 12-to-55-years-old (at visit between 2015 and 2016) #####
  filter(
    between(as.duration(as_date('2015-01-01')-birth_date)/dyears(1),12,55) |
    between(as.duration(as_date('2016-12-31')-birth_date)/dyears(1),12,55)
  ) %>%
  lapply(X=1,Y=.,function(X,Y){
    # Record this step to the table of subject selection.
    readRDS('data/selection.rds') %>%
      rbind(
        data.frame(
          step='age selection by 12 to 55 years old'
          ,exc_visit=
            .$inc_visit[nrow(.)]
            -sum(public$visits$subject_id %in% Y$subject_id)
          ,inc_visit=sum(public$visits$subject_id %in% Y$subject_id)
          ,exc_subject=.$inc_subject[nrow(.)]-nrow(Y)
          ,inc_subject=nrow(Y)
        )
      ) %>%
      saveRDS('data/selection.rds')
    Y
  }) %>%
  .[[1]] %>%

  ##### females #####
  filter(sex=='female') %>%
  lapply(X=1,Y=.,function(X,Y){
    # Record this step to the table of subject selection.
    readRDS('data/selection.rds') %>%
      rbind(
        data.frame(
          step='sex selection by female'
          ,exc_visit=
            .$inc_visit[nrow(.)]
            -sum(public$visits$subject_id %in% Y$subject_id)
          ,inc_visit=sum(public$visits$subject_id %in% Y$subject_id)
          ,exc_subject=.$inc_subject[nrow(.)]-nrow(Y)
          ,inc_subject=nrow(Y)
        )
      ) %>%
      saveRDS('data/selection.rds')
    Y
  }) %>%
  .[[1]] %>%

  ##### who visit healthcare providers #####
  filter(subject_id%in%public$visits$subject_id) %>%
  lapply(X=1,Y=.,function(X,Y){
    # Record this step to the table of subject selection.
    readRDS('data/selection.rds') %>%
      rbind(
        data.frame(
          step='visit healthcare providers'
          ,exc_visit=
            .$inc_visit[nrow(.)]
            -sum(public$visits$subject_id %in% Y$subject_id)
          ,inc_visit=sum(public$visits$subject_id %in% Y$subject_id)
          ,exc_subject=.$inc_subject[nrow(.)]-nrow(Y)
          ,inc_subject=nrow(Y)
        )
      ) %>%
      saveRDS('data/selection.rds')
    Y
  }) %>%
  .[[1]]
```

```{r Determine pregnant women of target population, eval=FALSE, include=FALSE}
public$pregnancy_status=
  
  # Get pregnancy status of the target population.
  public$target_population %>%
  lapply(X=1,Y=.,Z=public$visits,K=public$annotation,function(X,Y,Z,K){
    
    # Use these regular expression for the target population
    # to find codes related to pregnancy.
    L=Z %>%
      filter(subject_id%in%Y$subject_id) %>%
      left_join(K,by='code') %>%
      filter(
        str_detect(
          str_to_lower(paste(code,desc))
          ,paste0(c(
              'obstet'
              ,'pregnan'
              ,'labou?r\\s+'
              ,'deliver'
              ,'cesar'
              ,'natal'
              ,'z3[3467]'
              ,'o\\d+'
            ),collapse='|')
        )
      )
    
    # From the pregnancy-related visits, find the earliest and the latest.
    M=L %>%
      group_by(subject_id) %>%
      summarize(
        preg_e=min(admission_date)
        ,preg_l=max(admission_date)
        ,.groups='drop'
      ) %>%
      ungroup()
    
    # Find the related codes for the end of pregnancy.
    N=L %>%
      
      # This may include the false-positive codes
      filter(
        str_detect(
          str_to_lower(paste(code,desc))
          ,paste0(c(
            'abort'
            ,'deliver'
            ,'cesar'
            ,'labou?r\\s+'
            ,'natal'
            ,'postpartum'
            ,'terminat'
            ,'o0[0123]'
            ,'o152'
            ,'o364'
            ,'o63'
            ,'o7[02]'
            ,'o8[579]'
            ,'o90'
            ,'z3[79]'
          ),collapse='|')
        )
      ) %>%
      
      # By manual inspection, exclude the false-positive codes.
      filter(!(
        str_detect(
          str_to_lower(paste(code,desc))
          ,paste0(c(
            'aborter'
            ,'ante'
            ,'peri'
            ,'delayed'
            ,'false'
            ,'failed'
            ,'prenatal'
            ,'threatened'
            ,'o311'
            ,'z351'
            ,'o600'
            ,'o96'
          ),collapse='|')
        ) &
        !str_detect(
          str_to_lower(code)
          ,paste0(c(
            'o0[34568]'
            ,'o15[12]'
            ,'o7[02]'
            ,'o8[79]'
          ),collapse='|')
        )
      )) %>%
      
      # Record the codes for the end of pregnancy. 
      lapply(X=1,Y=.,function(X,Y){
        saveRDS(Y,'data/termination_codes.rds')
        Y
      }) %>%
      .[[1]] %>%
      
      # Find the earliest and latest dates of 
      # the related codes for  the end of pregnancy.
      group_by(subject_id) %>%
      summarize(
        termin_e=min(admission_date)
        ,termin_l=max(admission_date)
        ,.groups='drop'
      ) %>%
      ungroup()
    
    # Get visits after the end of the first pregnancy in the dataset period.
    O=L %>%
      left_join(N,by='subject_id') %>%
      filter(admission_date>termin_l)
    
    # Get the earliest date of the second-pregnancy visits.
    P=O %>%
      group_by(subject_id) %>%
      summarize(
        preg_e2=min(admission_date)
        ,.groups='drop'
      ) %>%
      ungroup()
    
    # Find the related codes for the end of the second pregnancy.
    Q=O %>%
      
      # This may include the false-positive codes
      filter(
        str_detect(
          str_to_lower(paste(code,desc))
          ,paste0(c(
            'abort'
            ,'deliver'
            ,'cesar'
            ,'labou?r\\s+'
            ,'natal'
            ,'postpartum'
            ,'terminat'
            ,'o0[0123]'
            ,'o152'
            ,'o364'
            ,'o63'
            ,'o7[02]'
            ,'o8[579]'
            ,'o90'
            ,'z3[79]'
          ),collapse='|')
        )
      ) %>%
      
      # By manual inspection, exclude the false-positive codes.
      filter(!(
        str_detect(
          str_to_lower(paste(code,desc))
          ,paste0(c(
            'aborter'
            ,'ante'
            ,'peri'
            ,'delayed'
            ,'false'
            ,'failed'
            ,'prenatal'
            ,'threatened'
            ,'o311'
            ,'z351'
            ,'o600'
            ,'o96'
          ),collapse='|')
        ) &
        !str_detect(
          str_to_lower(code)
          ,paste0(c(
            'o0[34568]'
            ,'o15[12]'
            ,'o7[02]'
            ,'o868'
            ,'o8[79]'
          ),collapse='|')
        )
      )) %>%
      
      # Find the earliest and latest dates of 
      # the related codes for  the end of the second pregnancy.
      group_by(subject_id) %>%
      summarize(
        termin_e2=suppressWarnings(min(admission_date))
        ,termin_l2=suppressWarnings(max(admission_date))
        ,.groups='drop'
      ) %>%
      ungroup()
    
    # Join offset dates to find the pregnancy periods.
    R=M %>%
      left_join(N,by='subject_id') %>%
      left_join(P,by='subject_id') %>%
      left_join(Q,by='subject_id') %>%
      select(
        subject_id
        ,preg_e
        ,termin_e
        ,termin_l
        ,preg_e2
        ,termin_e2
        ,termin_l2
        ,preg_l
        ,everything()
      )
    
    # Join the offset dates to the target population data
    # and compute the number of either gestation and termination.
    S=Y %>%
      left_join(R,by='subject_id') %>%
      mutate(
        gestation=
          as.integer(!is.na(preg_e))
          + as.integer(!is.na(preg_e2))
        ,termination=
          as.integer(!is.na(termin_l))
          + as.integer(!is.na(termin_l2))
      )
    
    # Filtered by gestation, select different offsets depending on
    # the pregnancy period, and combine the filtering results. 
    # This separates 2 pregnancy periods of a subject into 2 instances.
    rbind(
        filter(S,gestation==0) %>%
          select(
            subject_id,preg_e,termin_e,termin_l,preg_l,gestation,termination
          ) %>%
          mutate(gestation_n=0)
        ,filter(S,gestation==1) %>%
          select(
            subject_id,preg_e,termin_e,termin_l,preg_l,gestation,termination
          ) %>%
          mutate(gestation_n=1)
        ,filter(S,gestation==2) %>%
          select(
            subject_id,preg_e,termin_e,termin_l,preg_l,gestation,termination
          ) %>%
          mutate(gestation_n=1)
        ,filter(S,gestation==2) %>%
          select(
            subject_id,preg_e2,termin_e2,termin_l2,preg_l,gestation,termination
          ) %>%
          rename(preg_e=preg_e2,termin_e=termin_e2,termin_l=termin_l2) %>%
          mutate(gestation_n=2)
      ) %>%
      
      # If no termination for a pregnancy period of a subject,
      # then label the period as having censored outcome.
      mutate(censoring=is.na(termin_l)) %>%
      
      # Join to the target population data and arranged by subject_id
      left_join(
        S %>%
          select(
            -preg_e
            ,-termin_e
            ,-termin_l
            ,-preg_e2
            ,-termin_e2
            ,-termin_l2
            ,-preg_l
            ,-gestation
            ,-termination
          )
        ,by='subject_id'
      ) %>%
      arrange(
        factor(subject_id,S$subject_id %>% .[!duplicated(.)])
      )
    
  }) %>%
  .[[1]]
```

```{r Save termination codes as CSV, eval=FALSE, include=FALSE}
readRDS('data/termination_codes.rds') %>%
  
  # By manual inspection, check the termination codes.
  select(code,desc) %>%
  filter(!duplicated(.)) %>%
  arrange(code) %>%
  
  rename(description=desc) %>%
  kable() %>%
  kable_classic()
```

```{r Determine outcome, eval=FALSE, include=FALSE}
public$outcome=
  
  # Filter visits from the target population only.
  public$visits %>%
  filter(subject_id %in% public$target_population$subject_id) %>%
  
  # Extract outcome, which is O42.
  extract_outcome(
    icd10_event='O42'
    ,latest_event=min
    ,day_to_event=0
    ,icd10_nonevent=''
    ,latest_nonevent=max
    ,day_to_nonevent=0
    ,verbose=0
  ) %>%
  
  # Join the pregnancy status data to the outcome data.
  left_join(public$pregnancy_status,by='subject_id') %>%
  
  # If non-event, the latest date should be 
  # the earliest date of the end of pregnancy.
  lapply(X=1,Y=.,function(X,Y){
    Z=Y %>%
      filter(!censoring & outcome=='nonevent') %>%
      mutate(latest_date=termin_e)
    K=Y %>%
      filter(!(!censoring & outcome=='nonevent'))
    rbind(Z,K) %>%
      arrange(factor(subject_id,unique(Y$subject_id)))
  }) %>%
  .[[1]] %>%
  
  # Get only column to filter visits before the outcome 
  # for each pregnancy episode.
  select(subject_id,latest_date,outcome,censoring,gestation_n)
```

```{r Determine target visits, eval=FALSE, include=FALSE}
public$target_visits=
  
  # Join the outcome data to the visit data.
  public$outcome %>%
  right_join(public$visits,by='subject_id') %>%
  select(visit_id, everything()) %>%
  
  # If the outcome is censored, take visits up to the outcome date.
  # Otherwise, take all visits.
  filter(!is.na(censoring)) %>%
  filter(censoring | (!censoring & admission_date<=latest_date)) %>%
  mutate(code=ifelse(is.na(code)|code=='','NA',code)) %>%
  
  # Take only codes that are available 
  # in all combinations of outcome and censoring status.
  lapply(X=1,Y=.,function(X,Y){
    Z=Y %>%
      select(outcome,censoring,code) %>%
      filter(!duplicated(.)) %>%
      mutate(count=1) %>%
      group_by(code) %>%
      summarize(count=sum(count)) %>%
      arrange(desc(count)) %>%
      filter(count==4) %>%
      pull(code)
    
    Y %>% filter(code%in%Z)
  }) %>%
  .[[1]] %>%
  
  # Take needed columns and make subject_id different 
  # between pregnancy episodes of the same subject.
  select(-censoring,-outcome,-latest_date) %>%
  unite(subject_id,subject_id,gestation_n,sep='.')
```

```{r Update related variables, eval=FALSE, include=FALSE}
# Take only subjects' outcome that are assigned to the target visits.
public$outcome2=
  public$outcome %>%
  unite(subject_id,subject_id,gestation_n,sep='.') %>%
  filter(subject_id%in%public$target_visits$subject_id)

# Take only subjects' pregnancy status that are assigned to the target visits.
public$pregnancy_status2=
  public$pregnancy_status %>%
  unite(subject_id,subject_id,gestation_n,sep='.') %>%
  filter(subject_id%in%public$target_visits$subject_id)

# Add more criterion to the target population.
public$target_population2=
  
  public$target_population %>%
  left_join(
    select(public$pregnancy_status,subject_id,gestation_n)
    ,by=c('subject_id')
  ) %>%
  unite(subject_id,subject_id,gestation_n,sep='.') %>%
  
  ##### before the latest date of event/non-event #####
  ##### and split if >1 pregnancies #####
  filter(subject_id%in%public$target_visits$subject_id) %>%
  lapply(X=1,Y=.,function(X,Y){
    # Record this step to the table of subject selection.
    step_desc='up to the latest date for uncensored and split if >1 pregnancies'
    readRDS('data/selection.rds') %>%
      filter(step!=step_desc) %>%
      rbind(
        data.frame(
          step=step_desc
          ,exc_visit=
            .$inc_visit[nrow(.)]
            -sum(public$target_visits$subject_id %in% Y$subject_id)
          ,inc_visit=sum(public$target_visits$subject_id %in% Y$subject_id)
          ,exc_subject=.$inc_subject[nrow(.)]-nrow(Y)
          ,inc_subject=nrow(Y)
        )
      ) %>%
      saveRDS('data/selection.rds')
    Y
  }) %>%
  .[[1]]
```

```{r Describe selection results for sanity check, eval=FALSE, include=FALSE}
readRDS('data/selection.rds') %>%
  kable() %>%
  kable_classic()
```

```{r Describe pregnancy status, eval=FALSE, include=FALSE}
public$pregnancy_status %>%
  
  # Summarize the number and proportion of subjects with 0 to 2 gestation.
  select(subject_id,gestation) %>%
  filter(!duplicated(.)) %>%
  group_by(gestation) %>%
  summarize(n=n(),.groups='drop') %>%
  mutate(p=n/sum(n)) %>%
  
  # Join to compare with the subjects' pregnancy status 
  # that are assigned to the target visits.
  left_join(
    public$pregnancy_status2 %>%
      separate(subject_id,c('subject_id','gestation_n'),sep='\\.') %>%
      select(subject_id,gestation) %>%
      filter(!duplicated(.)) %>%
      group_by(gestation) %>%
      summarize(n2=n(),.groups='drop') %>%
      mutate(p2=n2/sum(n2))
    ,by='gestation'
  ) %>%
  kable() %>%
  kable_classic()
```

```{r Describe outcome per pregnancy status, eval=FALSE, include=FALSE}
public$pregnancy_status %>%
  
  # Join the ourcome data to the pregnancy status data.
  left_join(public$outcome,by=c('subject_id','gestation_n','censoring')) %>%
  
  # Summarize the number and proportion of subjects 
  # by combination of censoring status, outcome, and number of gestation.
  group_by(gestation_n,outcome,censoring) %>%
  summarize(n=n(),.groups='drop') %>%
  mutate(p=n/sum(n)) %>%
  
  # Join to compare with the subjects' pregnancy status 
  # that are assigned to the target visits.
  left_join(
    public$pregnancy_status2 %>%
      separate(subject_id,c('subject_id','gestation_n'),sep='\\.') %>%
      left_join(
        public$outcome2 %>%
          separate(subject_id,c('subject_id','gestation_n'),sep='\\.')
        ,by=c('subject_id','gestation_n','censoring')
      ) %>%
      mutate(gestation_n=as.numeric(gestation_n)) %>%
      group_by(gestation_n,outcome,censoring) %>%
      summarize(n2=n(),.groups='drop') %>%
      mutate(p2=n2/sum(n2))
    ,by=c('gestation_n','outcome','censoring')
  ) %>%
  kable() %>%
  kable_classic()
```

```{r Save selection results to RDS file, eval=FALSE, include=FALSE}
saveRDS(public$target_population2,'data/target_population.rds')
saveRDS(public$pregnancy_status2,'data/pregnancy_status.rds')
saveRDS(public$outcome2,'data/outcome.rds')
saveRDS(public$target_visits,'data/target_visits.rds')
saveRDS(public$annotation,'data/annotation.rds')
```

```{r Remove public data II after selection, eval=FALSE, include=FALSE}
rm(public)
```

We conducted data preprocessing after defining the target population and 
sampling it retrospectively. Demographics were included as categorical 
variables for causal factor we used as variable of interests. Then, We computed 
a number of days for a code, or any code representing a causal factor, in the 
latest encounter before each visit.

```{r Load an outcome table, include=FALSE}
outcome=
  readRDS('data/outcome.rds') %>%
  left_join(
    readRDS('data/pregnancy_status.rds')
    ,by=c('subject_id','censoring')
  ) %>%
  rename(reghc_id=healthcare_id)
```

```{r Visits with categorical identity as additional variables, include=FALSE}
visit_cip=
  
  # Transform age into a categorical variable based on domain knowledge.
  outcome %>%
  mutate(
    age=case_when(
      ((latest_date-birth_date)/dyears(1))<20~'Too Young'
      ,(((latest_date-birth_date)/dyears(1))>=20 &
        ((latest_date-birth_date)/dyears(1))<=35)
       ~'Low Risk'
      ,((latest_date-birth_date)/dyears(1))>35~'Too Old'
      ,TRUE~'NA'
    )
  ) %>%
  
  # Select subject_id and all categorical variables (not medical history).
  select(subject_id,age,marital_status,insurance_class,occupation_segment) %>%
  gather(variable,value,-subject_id) %>%
  unite(desc,variable,value,sep='_') %>%
  mutate(desc=str_replace_all(desc,'_|-|\\s',' ')) %>%
  separate(desc,c('first_word','desc'),sep=' ',extra='merge') %>%
  mutate_at('first_word',str_to_sentence) %>%
  unite(desc,first_word,desc,sep=' ') %>%
  
  # Create a code for each category.
  lapply(X=1,Y=.,function(X,Y){
    select(.,desc) %>%
      filter(!duplicated(.)) %>%
      
      # Create a code for each categorical variable.
      mutate(code=sapply(desc,function(x){
        str_split(x,'\\s')[[1]] %>%
          substr(1,1) %>%
          paste(collapse='') %>%
          str_to_upper()
      })) %>%
      
      # For each code, assign number to non-single code.
      group_by(code) %>%
      mutate(code_seq=seq(n()),max_code_seq=n()) %>%
      ungroup() %>%
      unite(code2,code,code_seq,sep='',remove=F) %>%
      mutate(code=ifelse(max_code_seq==1,code,code2)) %>%
      
      # Save all categorical variables and the codes.
      select(code,desc) %>%
      saveRDS('data/cat_identity.rds')
    Y
  }) %>%
  .[[1]] %>%
  
  # Join the codes of the categorical variables.
  left_join(readRDS('data/cat_identity.rds'),by='desc') %>%
  
  # Join the target visits.
  left_join(
    readRDS('data/target_visits.rds') %>%
      select(-code) %>%
      mutate(seq=seq(nrow(.))) %>%
      select(seq,everything())
    ,by='subject_id'
  ) %>%
  
  # Select columns according to the standard for medhist.
  select(
    seq
    ,visit_id
    ,subject_id
    ,healthcare_id
    ,admission_date
    ,code
    ,db_start_date
  ) %>%
  
  # Add the categorical variables as if these are diagnosis/procedure codes 
  # of which dates are the dates of any codes in a visit of a subject.
  rbind(
    readRDS('data/target_visits.rds') %>%
      mutate(seq=seq(nrow(.))) %>%
      select(seq,everything())
  )
```

```{r Determine measurement of causal factors, include=FALSE}
# Outcome with the causal factors based on prior knowledge
baseline_nodes=
  matrix(
    c('name','label'
      ,'PROM','Y01'
      ,'Multiple pregnancy','A02'
      ,'Chorioamnionitis','A03'
      ,'IAI','A04'
      ,'APH','A06'
      ,'GTI','A10'
      ,'Periodontal disease','A11'
      ,'Pneumonia','A13'
      ,'Asthma','A15'
      ,'Low SES','A19'
      ,'Maternal age','A20'
      ,'Influenza','A28'
      )
    ,ncol=2,byrow=T
  ) %>%
  `colnames<-`(.[1,]) %>%
  .[-1,] %>%
  as.data.frame()

# Define components of each causal factor by ICD-10 coding.
measure_nodes=
  baseline_nodes %>%
  mutate(
    regex=case_when(
      name=='PROM'~'O42'
      ,name=='Multiple pregnancy'~'O3[01]'
      ,name=='Chorioamnionitis'~'O411'
      ,name=='IAI'~'O41[89]'
      ,name=='APH'~'O46'
      ,name=='GTI'~'914[139]|A6[03]|O23[59]|R87'
      ,name=='Periodontal disease'~'K05'
      ,name=='Pneumonia'~'J1[23458]'
      ,name=='Asthma'~'J4[56]'
      ,name=='Low SES'~'THIRD|UNEMPLOYED'
      ,name=='Maternal age'~'TOO YOUNG|TOO OLD'
      ,name=='Influenza'~'J(00|09|10|11)'
      ,TRUE~''
    )
  ) %>%
  filter(regex!='') %>%
  lapply(X=seq(nrow(.)),Y=.,function(X,Y){
    data.frame(name=Y$regex[X],label=paste0(Y$label[X],'*'))
  }) %>%
  do.call(rbind,.)
```

```{r Compute nationwide day interval of medical history, include=FALSE}
if(run_heavy_computation){
  mh_nationwide=
    readRDS('data/target_visits.rds') %>%
    mutate(healthcare_id='nationwide') %>%
    extract_medical_history(cl=detectCores()-1)
  saveRDS(mh_nationwide,'data/mh_nationwide.rds')
}else{
  cat(readRDS('data/log.rds')[['mh_nationwide']])
  mh_nationwide=readRDS('data/mh_nationwide.rds')
}
```

```{r Compute provider-wise day interval of medical history, include=FALSE}
if(run_heavy_computation){
  mh_provider=
    readRDS('data/target_visits.rds') %>%
    extract_medical_history(cl=detectCores()-1)
  saveRDS(mh_provider,'data/mh_provider.rds')
}else{
  cat(readRDS('data/log.rds')[['mh_provider']])
  mh_provider=readRDS('data/mh_provider.rds')
}
```

```{r Compute nationwide day interval of causal factor, include=FALSE}
if(run_heavy_computation){
  cf_nationwide=
    
    # Exclude outcome.
    measure_nodes %>%
    filter(label!='Y01*') %>%
    
    # For each causal factor, filter rows with the codes related to the factor 
    # regardless the healthcare facilities.
    lapply(X=seq(nrow(.))
           ,Y=.
           ,Z=
             visit_cip %>%
             mutate(healthcare_id='nationwide') %>%
             left_join(
               readRDS('data/cat_identity.rds') %>%
                 rbind(annotation)
               ,by='code'
             )
           ,function(X,Y,Z){
      
      if(Y$label[X]%in%c('A19*','A20*')){
        K=Z %>%
          filter(!str_detect(str_to_upper(code),'[:digit:]')) %>%
          filter(str_detect(str_to_upper(desc),Y$name[X]))
      }else{
        K=Z %>% filter(str_detect(str_to_upper(code),Y$name[X]))
      }
      
      K %>%
        select(-desc) %>%
        mutate(
          code=
            Y$label[X] %>%
            str_remove_all('\\*')
        )
      
    }) %>%
    do.call(rbind,.) %>%
    filter(!duplicated(.)) %>%
    select(-seq) %>%
    
    # Extract the day intervals.
    extract_medical_history(cl=detectCores()-1) %>%
    filter(!duplicated(.)) %>%
    
    # Join to the target visits.
    right_join(
      readRDS('data/target_visits.rds') %>%
        select(-code) %>%
        mutate(healthcare_id='nationwide')
      ,by=c('visit_id'
            ,'subject_id'
            ,'healthcare_id'
            ,'admission_date'
            ,'db_start_date')
    ) %>%
    select(
      visit_id
      ,subject_id
      ,healthcare_id
      ,admission_date
      ,db_start_date
      ,everything()
    )
  
  saveRDS(cf_nationwide,'data/cf_nationwide.rds')
}else{
  cat(readRDS('data/log.rds')[['cf_nationwide']])
  cf_nationwide=readRDS('data/cf_nationwide.rds')
}
```

```{r Compute provider-wise day interval of causal factor, include=FALSE}
if(run_heavy_computation){
  cf_provider=
    
    # Exclude outcome.
    measure_nodes %>%
    filter(label!='Y01*') %>%
    
    # For each causal factor, filter rows with the codes related to the factor 
    # in each of the healthcare facilities.
    lapply(X=seq(nrow(.))
           ,Y=.
           ,Z=
             visit_cip %>%
             left_join(
               readRDS('data/cat_identity.rds') %>%
                 rbind(annotation)
               ,by='code'
             )
           ,function(X,Y,Z){
      
      if(Y$label[X]%in%c('A19*','A20*')){
        K=Z %>%
          filter(!str_detect(str_to_upper(code),'[:digit:]')) %>%
          filter(str_detect(str_to_upper(desc),Y$name[X]))
      }else{
        K=Z %>% filter(str_detect(str_to_upper(code),Y$name[X]))
      }
      
      K %>%
        select(-desc) %>%
        mutate(
          code=
            Y$label[X] %>%
            str_remove_all('\\*')
        )
      
    }) %>%
    do.call(rbind,.) %>%
    filter(!duplicated(.)) %>%
    select(-seq) %>%
    
    # Extract the day intervals.
    extract_medical_history(cl=detectCores()-1) %>%
    filter(!duplicated(.)) %>%
    
    # Join to the target visits.
    right_join(
      readRDS('data/target_visits.rds') %>%
        select(-code)
      ,by=c('visit_id'
            ,'subject_id'
            ,'healthcare_id'
            ,'admission_date'
            ,'db_start_date')
    ) %>%
    select(
      visit_id
      ,subject_id
      ,healthcare_id
      ,admission_date
      ,db_start_date
      ,everything()
    )
  
  saveRDS(cf_provider,'data/cf_provider.rds')
}else{
  cat(readRDS('data/log.rds')[['cf_provider']])
  cf_provider=readRDS('data/cf_provider.rds')
}
```

```{r Sanity check for the number of visits and subjects, include=FALSE}
source('R/outcome_visit_subject-function.R')
```

```{r Show sanity check results, eval=FALSE, include=FALSE}
rbind(
    readRDS('data/target_visits.rds') %>%
      left_join(readRDS('data/outcome.rds'),by='subject_id') %>%
      outcome_visit_subject('original')
    
    ,mh_nationwide %>%
      left_join(readRDS('data/outcome.rds'),by='subject_id') %>%
      outcome_visit_subject('nationwide')
    
    ,mh_provider %>%
      left_join(readRDS('data/outcome.rds'),by='subject_id') %>%
      outcome_visit_subject('provider')
    
    ,cf_nationwide %>%
      left_join(readRDS('data/outcome.rds'),by='subject_id') %>%
      outcome_visit_subject('nationwide, causal')
    
    ,cf_provider %>%
      left_join(readRDS('data/outcome.rds'),by='subject_id') %>%
      outcome_visit_subject('provider, causal')
  ) %>%
  kable() %>%
  kable_classic()
```

```{r Build a function to personalize PROM tidy set, include=FALSE}
source('R/prom_tidyset_personalization-function.R')
```

To ensure historical rates defined by derivation set only, we need to conduct 
data partition before continuing the downstream analysis. Therefore, historical 
rates were not derived by involving validation set.

```{r Build a function to split for external validation, include=FALSE}
source('R/extv-function.R')
```

```{r Compile tidy sets followed by data partitioning, include=FALSE}
if(run_heavy_computation){
  
  # Nationwide data for inference by medical histories with splitting 
  # information for external validation
  inferdata=
    mh_nationwide %>%
    compile_mh_outcome(select(outcome,subject_id,latest_date,outcome)) %>%
    prom_tidyset_personalization(
      outcome=outcome
      ,experimenter=
        MIAME(
          name='Herdiantri Sufriyana'
          ,lab='Emily Chia-Yu Su Lab'
          ,contact='herdiantrisufriyana@unusa.ac.id'
          ,title='Medical history dataset for causal inference'
          ,abstract=
            str_replace_all(
              'This dataset is a medical-history table from the BPJS Kesehatan.
              The target population is health insurance holders of 
              12-to-55-years-old females who visit healthcare providers between 
              2015 and 2016. The outcome of interest is prelabor rupture of 
              membrane (PROM). The medical history scenario is recorded across
              healthcare providers nationwide.'
              ,'\n',' '
            ) %>%
            str_replace_all('\\s+',' ')
          ,url='https://github.com/herdiantrisufriyana/prom'
        )
      ,annotation=readRDS('data/annotation.rds')
    ) %>%
    extv(geo_p=0.12,tem_p=0.35,bgt_p=1,ran_p=0.2)
  saveRDS(inferdata,'data/inferdata.rds')
  
  # Provider-wise data for prediction by medical histories with splitting 
  # information for external validation
  predidata=
    mh_provider %>%
    compile_mh_outcome(select(outcome,subject_id,latest_date,outcome)) %>%
    prom_tidyset_personalization(
      outcome=outcome
      ,experimenter=
        MIAME(
          name='Herdiantri Sufriyana'
          ,lab='Emily Chia-Yu Su Lab'
          ,contact='herdiantrisufriyana@unusa.ac.id'
          ,title='Medical history dataset for prediction'
          ,abstract=
            str_replace_all(
              'This dataset is a medical-history table from the BPJS Kesehatan.
              The target population is health insurance holders of 
              12-to-55-years-old females who visit healthcare providers between 
              2015 and 2016. The outcome of interest is prelabor rupture of 
              membrane (PROM). The medical history scenario is recorded
              individually within each healthcare provider nationwide.'
              ,'\n',' '
            ) %>%
            str_replace_all('\\s+',' ')
          ,url='https://github.com/herdiantrisufriyana/prom'
        )
      ,annotation=readRDS('data/annotation.rds')
    ) %>%
    extv(geo_p=0.12,tem_p=0.35,bgt_p=1,ran_p=0.2)
  saveRDS(predidata,'data/predidata.rds')
  
  # Nationwide data for inference by causal factors with splitting information 
  # for external validation
  infercause=
    cf_nationwide %>%
    compile_mh_outcome(select(outcome,subject_id,latest_date,outcome)) %>%
    prom_tidyset_personalization(
      outcome=outcome
      ,experimenter=
        MIAME(
          name='Herdiantri Sufriyana'
          ,lab='Emily Chia-Yu Su Lab'
          ,contact='herdiantrisufriyana@unusa.ac.id'
          ,title='Medical history dataset for causal inference'
          ,abstract=
            str_replace_all(
              'This dataset is a medical-history table from the BPJS Kesehatan.
              The codes are re-assigned into causal factors (defined by regex).
              The target population is health insurance holders of 
              12-to-55-years-old females who visit healthcare providers between 
              2015 and 2016. The outcome of interest is prelabor rupture of 
              membrane (PROM). The medical history scenario is recorded across
              healthcare providers nationwide.'
              ,'\n',' '
            ) %>%
            str_replace_all('\\s+',' ')
          ,url='https://github.com/herdiantrisufriyana/prom'
        )
      ,annotation=
        measure_nodes %>%
        mutate(label=str_remove_all(label,'\\*')) %>%
        rename(code=label,desc=name) %>%
        select(code,desc)
    ) %>%
    extv(geo_p=0.12,tem_p=0.35,bgt_p=1,ran_p=0.2)
  saveRDS(infercause,'data/infercause.rds')
  
  # Provider-wise data for prediction by causal factors with splitting 
  # information for external validation
  predicause=
    cf_provider %>%
    compile_mh_outcome(select(outcome,subject_id,latest_date,outcome)) %>%
    prom_tidyset_personalization(
      outcome=outcome
      ,experimenter=
        MIAME(
          name='Herdiantri Sufriyana'
          ,lab='Emily Chia-Yu Su Lab'
          ,contact='herdiantrisufriyana@unusa.ac.id'
          ,title='Medical history dataset for prediction'
          ,abstract=
            str_replace_all(
              'This dataset is a medical-history table from the BPJS Kesehatan.
              The codes are re-assigned into causal factors (defined by regex).
              The target population is health insurance holders of 
              12-to-55-years-old females who visit healthcare providers between 
              2015 and 2016. The outcome of interest is prelabor rupture of 
              membrane (PROM). The medical history scenario is recorded
              individually within each healthcare provider nationwide.'
              ,'\n',' '
            ) %>%
            str_replace_all('\\s+',' ')
          ,url='https://github.com/herdiantrisufriyana/prom'
        )
      ,annotation=
        measure_nodes %>%
        mutate(label=str_remove_all(label,'\\*')) %>%
        rename(code=label,desc=name) %>%
        select(code,desc)
    ) %>%
    extv(geo_p=0.12,tem_p=0.35,bgt_p=1,ran_p=0.2)
  saveRDS(predicause,'data/predicause.rds')
  
}else{
  inferdata=readRDS('data/inferdata.rds')
  predidata=readRDS('data/predidata.rds')
  infercause=readRDS('data/infercause.rds')
  predicause=readRDS('data/predicause.rds')
}
```

```{r Show data partition, eval=FALSE, include=FALSE}
inferdata %>%
  lapply(X=1,Y=.,function(X,Y){
    
    # Use phenotype and protocol data.
    cbind(
        pData(phenoData(Y))
        ,pData(protocolData(Y))
      ) %>%
      separate(subject_id,c('subject_id','gestation_n'),sep='\\.') %>%
      select(gestation_n,outcome,censoring,geo,tem,bgt,ran,int) %>%
      gather(partition,value,-gestation_n,-outcome,-censoring) %>%
      filter(value) %>%
      
      # Summarize number of instances 
      # by outcome, censoring status, and partition.
      group_by(gestation_n,outcome,censoring,partition) %>%
      summarize(n=n(),.groups='drop') %>%
      
      # Compute the statistics.
      group_by(partition) %>%
      mutate(subtotal=sum(n)) %>%
      ungroup() %>%
      mutate(total=ncol(Y)) %>%
      mutate(p=subtotal/total) %>%
      
      # Wrap up.
      select(partition,subtotal,total,p) %>%
      filter(!duplicated(.)) %>%
      arrange(factor(partition,c('int','ran','geo','tem','bgt')))
  }) %>%
  .[[1]] %>%
  kable() %>%
  kable_classic()
```

Deriving historical rates is relatively time-consuming. If one eventually uses 
this method for predictive modeling, it is efficient to conduct any filtering 
of predictors before derivation of historical rates. Therefore, more time can 
be saved and one may not need to run expensive computation.

All candidate predictors, including non-demographical causal factors, have 
non-zero variances. There were 460 candidate predictors fulfilling this 
criterion. We also showed in the same table that there are 426 candidate 
predictors without perfect separation.

```{r Combine selected causal factors and medical histories, include=FALSE}
# Nationwide data for inference by causal factors,
# confirmed by IPW, and by medical histories
inferboth=
  ExpressionSet(
    assayData=
      rbind(
        exprs(infercause) %>%
          `rownames<-`(paste0('causal_',rownames(.)))
        ,exprs(inferdata)
      )
    ,phenoData=phenoData(inferdata)
    ,featureData=
      rbind(
        fData(infercause) %>%
          `rownames<-`(paste0('causal_',rownames(.)))
        ,fData(inferdata)
      ) %>%
      AnnotatedDataFrame(varMetadata=fvarMetadata(inferdata))
    ,experimentData=
      MIAME(
        name='Herdiantri Sufriyana'
        ,lab='Emily Chia-Yu Su Lab'
        ,contact='herdiantrisufriyana@unusa.ac.id'
        ,title='Medical history dataset for causal inference'
        ,abstract=
          str_replace_all(
            'This dataset is a medical-history table from the BPJS Kesehatan.
            Selected causal factors, that is re-assigned by regex, are added.
            The target population is health insurance holders of 
            12-to-55-years-old females who visit healthcare providers between 
            2015 and 2016. The outcome of interest is prelabor rupture of 
            membrane (PROM). The medical history scenario is recorded
            individually within each healthcare provider nationwide.'
            ,'\n',' '
          ) %>%
          str_replace_all('\\s+',' ')
        ,url='https://github.com/herdiantrisufriyana/prom'
      )
    ,annotation=annotation(inferdata)
    ,protocolData=protocolData(inferdata)
  )

# Provider-wise data for prediction by causal factors,
# confirmed by IPW, and by medical histories
prediboth=
  ExpressionSet(
    assayData=
      rbind(
        exprs(predicause) %>%
          `rownames<-`(paste0('causal_',rownames(.)))
        ,exprs(predidata)
      )
    ,phenoData=phenoData(predidata)
    ,featureData=
      rbind(
        fData(predicause) %>%
          `rownames<-`(paste0('causal_',rownames(.)))
        ,fData(predidata)
      ) %>%
      AnnotatedDataFrame(varMetadata=fvarMetadata(predidata))
    ,experimentData=
      MIAME(
        name='Herdiantri Sufriyana'
        ,lab='Emily Chia-Yu Su Lab'
        ,contact='herdiantrisufriyana@unusa.ac.id'
        ,title='Medical history dataset for prediction'
        ,abstract=
          str_replace_all(
            'This dataset is a medical-history table from the BPJS Kesehatan.
            Selected causal factors, that is re-assigned by regex, are added.
            The target population is health insurance holders of 
            12-to-55-years-old females who visit healthcare providers between 
            2015 and 2016. The outcome of interest is prelabor rupture of 
            membrane (PROM). The medical history scenario is recorded
            individually within each healthcare provider nationwide.'
            ,'\n',' '
          ) %>%
          str_replace_all('\\s+',' ')
        ,url='https://github.com/herdiantrisufriyana/prom'
      )
    ,annotation=annotation(predidata)
    ,protocolData=protocolData(predidata)
  )
```

```{r Candidate predictors with non-zero variances, eval=FALSE, include=FALSE}
var_candidate_predictors=
  
  # Use only training set.
  prediboth %>%
  .[,pData(protocolData(.))$int] %>%
  
  # Get predictors and the outcome.
  lapply(X=1,Y=.,function(X,Y){
    exprs(Y) %>%
      t() %>%
      as.data.frame() %>%
      cbind(select(pData(Y),'outcome'))
  }) %>%
  .[[1]] %>%
  
  # Summarize a standard deviation for each predictor per outcome.
  select(outcome,everything()) %>%
  gather(key,value,-outcome) %>%
  group_by(key,outcome) %>%
  summarize(sd_value=sd(value,na.rm=T),.groups='drop') %>%
  arrange(factor(key,unique(key)),outcome) %>%
  spread(outcome,sd_value) %>%
  setNames(str_remove_all(names(.),'-')) %>% 
  arrange(factor(key,rownames(prediboth)[rownames(prediboth)%in%key]))
```

```{r Train-set-based NPS predictors excluding outcome leaker, include=FALSE}
# Select predictors without perfect separation problem.
if(run_heavy_computation){
  
  int_nps=
    prediboth %>%
    .[,pData(protocolData(.))$int] %>%
    extract_nps_mh() %>%
    arrange(factor(key,rownames(prediboth)[rownames(prediboth)%in%key]))
  
  saveRDS(int_nps,'data/int_nps.rds')
  
}else{
  int_nps=readRDS('data/int_nps.rds')
}

# Identify codes related to the end of pregnancy.
outcome_leaker=
  int_nps %>%
  left_join(rename(annotation,key=code),by='key') %>%
  filter(
    str_detect(
      str_to_lower(paste(key,desc))
      ,paste0(c(
        'abort'
        ,'deliver'
        ,'cesar'
        ,'labou?r\\s+'
        ,'natal'
        ,'postpartum'
        ,'terminat'
        ,'o0[0123]'
        ,'o152'
        ,'o364'
        ,'o63'
        ,'o7[02]'
        ,'o8[579]'
        ,'o90'
        ,'z3[79]'
      ),collapse='|')
    )
  ) %>%
  filter(!(
    str_detect(
      str_to_lower(paste(key,desc))
      ,paste0(c(
        'aborter'
        ,'ante'
        ,'peri'
        ,'delayed'
        ,'false'
        ,'failed'
        ,'prenatal'
        ,'threatened'
        ,'o311'
        ,'z351'
        ,'o600'
        ,'o96'
      ),collapse='|')
    ) &
    !str_detect(
      str_to_lower(key)
      ,paste0(c(
        'o0[34568]'
        ,'o15[12]'
        ,'o7[02]'
        ,'o8[79]'
      ),collapse='|')
    )
  )) %>%
  select(key,desc)

# Exclude the outcome-leaker codes.
int_nps_eol_p=
  int_nps %>%
  left_join(
    outcome_leaker %>%
      select(key) %>%
      mutate(excluded=1)
    ,by='key'
  ) %>%
  filter(is.na(excluded)) %>%
  select(-excluded)
```

```{r Non-zero variance and no perfect separation, eval=FALSE, include=FALSE}
var_candidate_predictors %>%
  
  # Exclude predictors that do not exist in training set.
  filter(!(nonevent==0 & event==0)) %>%
  
  # Join predictors without perfect separation problem.
  left_join(
    prediboth %>%
      .[,pData(protocolData(.))$int] %>%
      extract_nps_mh() %>%
      arrange(factor(key,rownames(prediboth)[rownames(prediboth)%in%key])) %>%
      mutate(perfect_separation='No')
    ,by=c('key','nonevent','event')
  ) %>%
  mutate(
    perfect_separation=
      ifelse(is.na(perfect_separation),'Yes',perfect_separation)
  ) %>%
  
  # Join combined annotation tables of medical histories and causal factors.
  left_join(
    rename(annotation,key=code) %>%
      rbind(
        dag$baseline_nodes %>%
          mutate(label=paste0('causal_',label)) %>%
          rename(desc=name,key=label) %>%
          select(key,desc)
      )
    ,by='key'
  ) %>%
  rename(candidate_predictor=key,description=desc) %>%
  setNames(str_to_title(colnames(.)) %>% str_replace_all('_',' ')) %>%
  
  kable() %>%
  kable_classic()
```

We excluded the diagnosis/procedure codes that may leak the outcome 
information. We only used the existing codes in the training set to determine 
outcome-leaker codes based on the previous codes for determining delivery or 
immediately after delivery care. There were 54 codes that may leak the outcome. 
All of them were also irredundant.

```{r Save outcome leakers, eval=FALSE, include=FALSE}
outcome_leaker %>%
  arrange(key) %>%
  rename(code=key,description=desc) %>%
  setNames(str_to_title(colnames(.))) %>%
  
  kable() %>%
  kable_classic()
```

```{r Correlations of candidate predictors, eval=FALSE, include=FALSE}
cor_predictors=
  
  # Use only training set.
  prediboth %>%
  .[,pData(protocolData(.))$int] %>%
  
  # Get predictors and the outcome.
  lapply(X=1,Y=.,function(X,Y){
    exprs(Y) %>%
      t() %>%
      as.data.frame() %>%
      cbind(select(pData(Y),'outcome'))
  }) %>%
  .[[1]] %>%
  
  # Get only predictors without perfect separation problem and not outcome leakers. 
  select(outcome,everything()) %>%
  mutate_all(function(x)ifelse(is.na(x),0,x)) %>%
  .[,int_nps_eol_p$key] %>%
  
  # Compute inter-predictor Pearson correlation coefficients.
  cor()
```

```{r Save and show the inter-predictor correlations, eval=FALSE, include=FALSE}
# Save the correlation coefficients.
cor_predictors %>%
  as.data.frame() %>%
  rownames_to_column(var=' ') %>%
  
  kable() %>%
  kable_classic()

# Show the correlation coefficients.
cor_predictors %>%
  
  # Remove auto-correlations.
  as.data.frame() %>%
  rownames_to_column(var='first') %>%
  gather(second,pearson_correlation_r,-first) %>%
  filter(first!=second) %>%
  
  # Remove redundant correlations.
  mutate(pair=seq(nrow(.))) %>%
  gather(order,predictor,-pair,-pearson_correlation_r) %>%
  group_by(pair) %>%
  arrange(pair,predictor) %>%
  mutate(order=c('first','second')) %>%
  ungroup() %>%
  spread(order,predictor) %>%
  select(-pair) %>%
  filter(!duplicated(.)) %>%
  
  # Filter the high-correlated pair of predictors.
  filter(pearson_correlation_r>0.7) %>%
  
  # Join regular expression of the coding components of causal factors 
  # to check if the high-correlated pairs are due to these components.
  left_join(
    dag$baseline_nodes %>%
      mutate(label=paste0('causal_',label)) %>%
      rename(first=label) %>%
      select(first,name)
    ,by='first'
  ) %>%
  left_join(
    dag$measure_nodes %>%
      mutate(label=paste0('causal_',str_remove_all(label,'\\*'))) %>%
      rename(regular_expression=name,first=label) %>%
      select(first,regular_expression)
    ,by='first'
  )
```

We also determined causal factors as the candidate predictors. These can be an 
example how to conduct the data transformation on a variable represented by 
multiple codes of diagnosis and procedure. We combined these factors with other 
variables that assign a single code.

```{r Save codes and demographics for causal factors, eval=FALSE, include=FALSE}
measure_nodes %>%
  
  # Get the regular expression to define causal factors.
  mutate(label=str_remove_all(label,'\\*')) %>%
  rename(regular_expression=name) %>%
  
  # Add the description.
  left_join(baseline_nodes,by='label') %>%
  filter(label!='Y01') %>%
  mutate(label=paste0('causal_',label)) %>%
  rename(causal_factor=name,variable=label) %>%
  select(causal_factor,variable,regular_expression) %>%
  
  # Based on the regular expression, gather the ICD-10 codes that apply.
  lapply(X=seq(nrow(.)),Y=.,function(X,Y){
    
    Z=Y$regular_expression[X]
    
    if(str_detect(Z,'\\(|\\)')){
      
      K=str_split(Z,'\\(|\\)')[[1]]
      
      L=K[2] %>%
        str_split('\\|') %>%
        .[[1]] %>%
        sapply(X=seq(length(.)),Y=.,Z=K[1],function(X,Y,Z){
          paste0(Z,Y[X])
        })
      
    }else{
      
      K=str_split(Z,'\\|')[[1]]
      
      L=K %>%
        lapply(X=seq(length(.)),Y=.,function(X,Y){
          if(str_detect(Y[X],'\\[|\\]')){
            
            Z=str_split(Y[X],'\\[|\\]')[[1]]
            
            if(str_detect(Z[2],'\\^')){
              
              K=Z[2] %>%
                str_remove_all('\\^')
              
              paste0(0:9,collapse='') %>%
                str_remove_all(K) %>%
                sapply(X=seq(str_count(.)),Y=.,Z=Z[1],function(X,Y,Z){
                  paste0(Z,str_sub(Y,X,X))
                })
              
            }else if(str_detect(Z[2],'\\-')){
              
              K=Z[2] %>%
                str_split_fixed('\\-',2)
              
              paste0(K[1]:K[2],collapse='') %>%
                sapply(X=seq(str_count(.)),Y=.,Z=Z[1],function(X,Y,Z){
                  paste0(Z,str_sub(Y,X,X))
                })
              
            }else{
              
              Z[2] %>%
                sapply(X=seq(str_count(.)),Y=.,Z=Z[1],function(X,Y,Z){
                  paste0(Z,str_sub(Y,X,X))
                })
              
            }
            
          }else{
            
            Y[X]
            
          }
        }) %>%
        unlist()
      
    }
    
    data.frame(
      causal_factor=Y$causal_factor[X]
      ,variable=Y$variable[X]
      ,regular_expression=Y$regular_expression[X]
      ,code=L
    )
    
  }) %>%
  do.call(rbind,.) %>%
  select(-regular_expression) %>%
  
  # Add information related to demographics.
  lapply(
    X=seq(nrow(.))
    ,Y=.
    ,Z=
      annotation %>%
      rbind(
        readRDS('data/cat_identity.rds') %>%
          mutate(
            code=
              desc %>%
              str_to_lower() %>%
              sapply(
                str_remove_all
                ,paste0(
                  c('age'
                    ,'householder'
                    ,str_replace_all(colnames(outcome),'_',' '))
                  ,collapse='|'
                )
              ) %>%
              str_to_upper() %>%
              trimws()
          )
      )
    ,function(X,Y,Z){
    
      Z %>%
        filter(str_detect(code,Y$code[X])) %>%
        mutate(
          causal_factor=Y$causal_factor[X]
          ,variable=Y$variable[X]
        )
  }) %>%
  do.call(rbind,.) %>%
  
  select(causal_factor,variable,everything()) %>%
  rename(demographics_or_medical_history=code,description=desc) %>%
  setNames(str_to_title(colnames(.)) %>% str_replace_all('_',' ')) %>%
  kable() %>%
  kable_classic()
```

We inferred the nationwide historical rates given the day number from a code 
encounter to current visit for each candidate predictor, as described in the 
main text. This used irredundant candidate predictors with non-zero variances 
and no perfect separation in training set only.

```{r Compute historical rate based on nationwide training set, include=FALSE}
if(run_heavy_computation){
  nw_int_hlin=
    inferboth %>%
    .[int_nps_eol_p$key
      ,pData(protocolData(.))$int] %>%
    trans_hist_rate(
      interpolation='linear'
      ,verbose=F
    )
  
  saveRDS(nw_int_hlin,'data/nw_int_hlin.rds')
}else{
  nw_int_hlin=readRDS('data/nw_int_hlin.rds')
}
```

The candidate predictors were transformed into the historical rates. We 
conducted this step in all data partitions within a provider-wise dataframe. 
But, we used the nationwide historical rates using derivation set only. If one 
uses this data transformation method for predictive modeling, the derivation 
set may be the same with a training set.

```{r Use the nationwide rate to get it for whole prediction set, include=FALSE}
if(run_heavy_computation){
  pw_hlin=
    prediboth %>%
    .[int_nps_eol_p$key,] %>%
    trans_hist_rate(
      hist_rate=preproc(nw_int_hlin)$hist_rate
      ,interpolation='linear'
      ,verbose=F
    )
  
  saveRDS(pw_hlin,'data/pw_hlin.rds')
}else{
  pw_hlin=readRDS('data/pw_hlin.rds')
}
```

```{r Show the outcome and censoring in training set, eval=FALSE, include=FALSE}
pw_hlin %>%
  .[,pData(protocolData(.))$int] %>%
  pData() %>%
  select(outcome,censoring) %>%
  table()
```

## Step 2 to 13 and 19

Previous data partition had not held out instances for calibration yet. This 
took 80% of training set. We also gave different weights for event and nonevent 
by including censored outcome. Hyperparameter tuning, final training, and 
calibration were conducted by bootstrapping for 30 times. The same resampling 
methods were applied for both classification and estimation tasks. Parallel 
computing by multiple central processing units (CPUs) were applied for 
calibrating the model.

```{r Prepare train set and parameters, include=FALSE}
# Create an empty list to save training parameters.
training_parameters=list()

# Hold out 20% of training set for calibration.
suppressWarnings(set.seed(66,sample.kind=sample.kind))
training_parameters$pre_calib_set=
  
  # Training set
  pw_hlin %>%
  .[,pData(protocolData(.))$int] %>%
  pData() %>%
  rownames_to_column(var='id') %>%
  
  # Uncensored outcome
  filter(!censoring) %>%
  
  # Get 80% for pre-calibrated set.
  lapply(X=1,Y=.,function(X,Y){
    Z=createDataPartition(Y$outcome,times=1,p=0.8)
    Y$id[Z$Resample1]
  }) %>%
  .[[1]]

# Compute outcome weights.
training_parameters$outcome_weights=
  
  # Training set
  pw_hlin %>%
  .[,pData(protocolData(.))$int] %>%
  pData() %>%
  select(outcome,censoring) %>%
  
  # Compute the probabilities, taking censoring into account.
  cbind(
    table(.) %>%
      as.numeric() %>%
      setNames(c('nonevent_uncensored','event_uncensored'
                 ,'nonevent_censored','event_censored')) %>%
      t() %>%
      as.data.frame() %>%
      mutate(
        total=
          nonevent_uncensored+
          nonevent_censored+
          event_uncensored+
          event_censored
        ,nonevent_prob=nonevent_uncensored/total
        ,event_prob=event_uncensored/total
      ) %>%
      select(nonevent_prob,event_prob)
  ) %>%
  rownames_to_column(var='id') %>%
  
  # Uncensored outcome
  filter(!censoring) %>%
  select(-censoring) %>%
  
  # Compute the weight from half od the inverse probability.
  mutate(
    weight=
      ifelse(
        outcome=='event'
        ,(1/event_prob)*0.5
        ,(1/nonevent_prob)*0.5
      )
  ) %>%
  column_to_rownames(var='id') %>%
  select(weight)

# Define classification training to apply 30-time bootstrapping.
training_parameters$final_trControl=
  trainControl(
    method='boot'
    ,number=30
    ,summaryFunction=twoClassSummary
    ,classProbs=T
    ,savePredictions=T
    ,allowParallel=T
  )

# Define estimation training to apply 30-time bootstrapping.
training_parameters$timereg_final_trControl=
  trainControl(
    method='boot'
    ,number=30
    ,savePredictions=F
    ,allowParallel=T
  )
```

```{r Empty lists to save models and the evaluation results, include=FALSE}
model=list()
calib_model=list()
eval_model=list()
timing_model=list()
eval_timing_model=list()
```

We used nationwide, pre-calibration training set to conduct feature selection 
by differential analysis. After quantile-to-quantile normalization, we 
conducted differential analysis as described in the main text. Only 
candidate predictors that showed adjusted *p*-values <0.05 were selected for 
1-bit stochastic gradient descent transformation.

Demanding different statistical assumption, we used unnormalized candidate 
predictors for creating a feature map (i.e. ontology array) and a network 
architecture (i.e. ontology network). Both procedures need a 
distance/similarity matrix. Standardization was applied by subtracting 
each value with feature-wise average and dividing it with feature-wise standard 
deviation. Then, we computed a feature-to-feature Pearson correlation matrix.

For creating a feature map, we projected the filtered candidate predictors onto 
three dimensions, as described in the main text. Meanwhile, using the same 
correlation matrix, we applied CliXO algorithm, as described in the main text. 
Each ontology array was fed to a block of neural network, i.e. 
Inception v4-Resnet.

```{r DI-VNN feature selection and representation, include=FALSE}
if(run_heavy_computation){
  input=list()
  
  # Use provider-wise, pre-calibrated, training set for modeling.
  input$pw=
    pw_hlin %>%
    .[,!pData(phenoData(.))$censoring] %>%
    .[,pData(protocolData(.))$int] %>%
    .[,colnames(.)%in%training_parameters$pre_calib_set,drop=F]
  
  # Use nationwide, pre-calibrated, training set for feature selection.
  input$nw=
    nw_int_hlin %>%
    .[,!pData(phenoData(.))$censoring] %>%
    .[,colnames(.)%in%training_parameters$pre_calib_set,drop=F]
  
  output=list()
  
  # Get uncensored outcome.
  output$outcome=
    input$pw %>%
    phenoData() %>%
    pData() %>%
    select(outcome)
  
  # Get features corresponding to uncensored outcome.
  output$raw$pw=
    input$pw %>%
    exprs() %>%
    t() %>%
    .[rownames(output$outcome),]
  
  output$raw$nw=
    input$nw %>%
    exprs() %>%
    t() %>%
    .[rownames(output$outcome),]
  
  # Select features utilizing differential analysis
  # by moderated-t stats with Benjamini-Hochberg multiple testing correction.
  output$fit=
    t(output$raw$nw) %>%
    normalize.quantiles() %>%
    `dimnames<-`(dimnames(t(output$raw$nw))) %>%
    lmFit(model.matrix(~outcome,output$outcome)) %>%
    eBayes() %>%
    topTable(coef=2,nrow(.$coefficients),adjust.method='BH',sort.by='none') %>%
    filter(adj.P.Val<0.05)
  
  # Get the selected features as unnormalized ones.
  output$unnorm$pw=
    output$raw$pw %>%
    .[,rownames(output$fit)]
  
  output$unnorm$nw=
    output$raw$nw %>%
    .[,rownames(output$fit)]
  
  # Normalize features quantile-to-quantile
  # using differential average of features as target.
  output$norm=
    output$unnorm$pw %>%
    t() %>%
    normalize.quantiles.use.target(output$fit$AveExpr) %>%
    t() %>%
    `dimnames<-`(dimnames(output$unnorm$pw))
  
  # Transform features by 1-bit stochastic gradient descent (SGD)
  # using the differential average.
  cat('Transform features by 1-bit stochastic gradient descent (SGD)\n')
  output$predictor_v=
    output$norm %>%
    sweep(2,output$fit$AveExpr,'-') %>%
    pbsapply(function(x){ifelse(x==0,0,ifelse(x>0,1,-1))}) %>%
    matrix(
      ncol=ncol(output$unnorm$pw)
      ,byrow=F
      ,dimnames=dimnames(output$unnorm$pw)
    ) %>%
    as.data.frame()
  
  # Get feature-wise mean of unnormalized features.
  output$predictor_m=
    output$unnorm$nw %>%
    colMeans2()
  
  # Get feature-wise standard deviation (SD) of unnormalized features.
  output$predictor_s=
    output$unnorm$nw %>%
    colSds()
  
  # Scale unnormalized features by the mean and SD.
  output$scaled=
    output$unnorm$nw %>%
    sweep(2,output$predictor_m,'-') %>%
    sweep(2,output$predictor_s,'/')
  
  # Compute feature-feature Pearson correlation matrix
  # of unnormalized features.
  cat('Compute feature-feature Pearson correlation matrix\n')
  output$predictor_p=
    colnames(output$unnorm$nw) %>%
    lapply(X=seq(length(.)-1),Y=.,function(X,Y){
      data.frame(
        predictor1=Y[X]
        ,predictor2=Y[(X+1):length(.)]
      )
    }) %>%
    do.call(rbind,.) %>%
    mutate(
      pearson=
        pbsapply(X=seq(nrow(.)),Y=.,Z=output$scaled,function(X,Y,Z){
          cor(
            Z[,Y$predictor1[X]]
            ,Z[,Y$predictor2[X]]
          )
        })
    ) %>%
    rbind(
      setNames(select(.,predictor2,predictor1,everything()),colnames(.))
      ,data.frame(
        predictor1=colnames(output$unnorm$nw)
        ,predictor2=colnames(output$unnorm$nw)
        ,pearson=1
      )
    ) %>%
    spread(predictor2,pearson) %>%
    column_to_rownames(var='predictor1') %>%
    as.matrix()
  
  # Conduct Barnes-Hut t-moderated stochastic neighbor embedding (t-SNE).
  cat('Conduct Barnes-Hut t-moderated stochastic neighbor embedding (t-SNE)\n')
  suppressWarnings(set.seed(33,sample.kind=sample.kind))
  output$predictor_tsne=
    output$predictor_p %>%
    Rtsne(dims=3,verbose=T,is_distance=T)
  
  # Construct clique-extracted ontology (CliXO) (choose your own OS).
  output$predictor_c=
    output$predictor_p %>%
    clixo(os='windows')
  
  # Compile input.
  input=list()
  
  input$value=output$predictor_v
  
  input$outcome=
    output$outcome %>%
    mutate(outcome=as.integer(outcome=='event')) %>%
    pull(outcome) %>%
    setNames(rownames(input$value))
  
  input$similarity=output$predictor_p
  
  input$mapping=
    output$predictor_tsne$Y %>%
    `rownames<-`(colnames(input$value))
  
  input$ontology=output$predictor_c
  
  input$fit=output$fit
  
  # Compile into a TidySet.
  cat('Compile into a TidySet\n')
  output=
    TidySet.compile(
      value=input$value
      ,outcome=input$outcome
      ,similarity=input$similarity
      ,mapping=input$mapping
      ,ontology=input$ontology
      ,ranked=T
      ,dims=7
      ,decreasing=F
      ,seed_num=33
    )
  
  saveRDS(input,'data/input.rds')
  saveRDS(output,'data/output.rds')
}else{
  cat(readRDS('data/log.rds')[['output_predictor_v']])
  cat(readRDS('data/log.rds')[['output_predictor_p']])
  cat(readRDS('data/log.rds')[['output_predictor_tsne']])
  cat(readRDS('data/log.rds')[['output']])
  input=readRDS('data/input.rds')
  output=readRDS('data/output.rds')
}
```

```{r Change outcome for timing regression, include=FALSE}
output_reg=
  output %>%
  `phenoData<-`(
    output %>%
      phenoData() %>%
      `pData<-`(
        output %>%
          pData() %>%
          rownames_to_column(var='id') %>%
          mutate(
            outcome=
              pw_hlin %>%
              .[,!pData(phenoData(.))$censoring] %>%
              .[,pData(protocolData(.))$int] %>%
              .[,colnames(.)%in%training_parameters$pre_calib_set,drop=F] %>%
              protocolData() %>%
              pData() %>%
              rownames_to_column(var='id') %>%
              mutate(
                timing=as.duration(latest_date-admission_date)/ddays(1)
              ) %>%
              select(id,timing) %>%
              pull(timing) %>%
              setNames(rownames(input$value))
          ) %>%
          column_to_rownames(var='id')
      )
  )
```

```{r Show ontonet, eval=FALSE, include=FALSE}
output %>%
  viz.ontonet(feature=F) %>%
  plot(
    node.size=5
    ,edge.arrow.size=0.25
    ,main='Untrained Ontonet'
  )
```

## Step 14 to 18 and 20

To get a representation, the computation is conducted by a series of 
iterations. A backpropagation algorithm was used each time. The computation was 
applied by the loss function, as described in the main text. 

```{r Create a function to refresh keras backend session, include=FALSE}
source('R/refresh_session-function.R')
refresh_session()
```

```{r Create a function to create training function given lambda, include=FALSE}
source('R/trainer_generator-function.R')
```

```{r Create a function to create regressor function given lambda, include=FALSE}
source('R/regressor_generator-function.R')
```

```{r Conduct hyperparameter tuning for a DI-VNN, include=FALSE}
lambda=10^seq(-10,-1,len=10)
if(run_heavy_computation){
  cat('Conduct hyperparameter tuning for DI-VNN model\n')
  cat('Started:',as.character(now()),'\n')
    
    surrogate_model=
      trainer_generator(
        output
        ,class_weight=
          pw_hlin %>%
          .[,colnames(.)%in%training_parameters$pre_calib_set,drop=F] %>%
          phenoData() %>%
          pData() %>%
          select(outcome) %>%
          mutate(outcome=as.integer(outcome=='event')) %>%
          cbind(
            training_parameters$outcome_weights %>%
            .[rownames(.)%in%training_parameters$pre_calib_set,,drop=F]
          ) %>%
          filter(!duplicated(.)) %>%
          arrange(outcome) %>%
          lapply(X=1,Y=.,function(X,Y){
            setNames(Y$weight,Y$outcome)
          }) %>%
          .[[1]]
        ,epochs=5
        ,patience=round(5/2)
        ,batch_size=512
        ,warm_up=0.05
        ,lr=2^-6
        ,min_lr=2^-6/512
        ,tuning_mode=T
        ,verbose=1
      )
    
    tuning_divnn=list()
    i=1
    for(j in seq(i,length(lambda))){
      suppressWarnings(set.seed(33,sample.kind=sample.kind))
      tuning_divnn[[j]]=surrogate_model(lambda[j])
    }
  
  rm(i,j)
  cat('End:',as.character(now()))
  saveRDS(tuning_divnn,'data/tuning_divnn.rds')
}else{
  cat(readRDS('data/log.rds')[['tuning_divnn']])
  tuning_divnn=readRDS('data/tuning_divnn.rds')
}
```

```{r Plot DI-VNN tuning results, eval=FALSE, include=FALSE}
tuning_divnn %>%
  sapply(function(x)min(x$Score)) %>%
  data.frame(mean_AUROC=.) %>%
  mutate(which_param=seq(nrow(.))) %>%
  mutate(best=which.max(mean_AUROC)) %>%
  qplot(which_param,mean_AUROC,color=which_param==best,data=.) +
  geom_smooth(method='loess',color='black',formula=y~x) +
  scale_x_continuous(breaks=seq(lambda))
```

```{r Conduct modeling for a DI-VNN, include=FALSE}
if(run_heavy_computation){
  cat('Conduct modeling for DI-VNN\n')
  cat('Started:',as.character(now()),'\n')
    
    surrogate_model=
      trainer_generator(
        output
        ,path='data/ontonet'
        ,class_weight=
          pw_hlin %>%
          .[,colnames(.)%in%training_parameters$pre_calib_set,drop=F] %>%
          phenoData() %>%
          pData() %>%
          select(outcome) %>%
          mutate(outcome=as.integer(outcome=='event')) %>%
          cbind(
            training_parameters$outcome_weights %>%
            .[rownames(.)%in%training_parameters$pre_calib_set,,drop=F]
          ) %>%
          filter(!duplicated(.)) %>%
          arrange(outcome) %>%
          lapply(X=1,Y=.,function(X,Y){
            setNames(Y$weight,Y$outcome)
          }) %>%
          .[[1]]
        ,epochs=500
        ,patience=100
        ,batch_size=512
        ,warm_up=0.05
        ,lr=2^-6
        ,min_lr=2^-6/512
        ,tuning_mode=F
        ,checkpoint=T
        ,verbose=1
      )
    suppressWarnings(set.seed(33,sample.kind=sample.kind))
    modeling_divnn=surrogate_model(lambda[10])
    
  cat('End:',as.character(now()))
  save_model_weights_hdf5(modeling_divnn$ontonet,'data/ontonet.h5')
  saveRDS(modeling_divnn,'data/modeling_divnn.rds')
}else{
  cat(readRDS('data/log.rds')[['modeling_divnn']])
  modeling_divnn=readRDS('data/modeling_divnn.rds')
  refresh_session()
  modeling_divnn$ontonet=
    output %>%
    # readLines('data/ontonet.json') %>%
    # model_from_json() %>%
    generator.ontonet(l2_norm=lambda[10]) %>%
    load_model_weights_hdf5('data/ontonet.h5') %>%
    # load_model_hdf5('data/entire_ontonet.h5',compile=T)
    compile(
      optimizer=optimizer_sgd(
        lr=modeling_divnn$history$metrics$lr %>%
          .[which.max(modeling_divnn$history$metrics$val_root_roc)]
        ,momentum=0.9
      )
      ,loss='mean_squared_error'
      ,loss_weights=c(rep(
          0.3/(0.3*(length(.$outputs)-1)+1),length(.$outputs)-1)
          ,1/(0.3*(length(.$outputs)-1)+1)
        )
      ,metrics=c(
          tf$keras$metrics$AUC(name='roc')
          ,tf$keras$metrics$TruePositives(name='tp')
          ,tf$keras$metrics$FalseNegatives(name='fn')
          ,tf$keras$metrics$FalsePositives(name='fp')
          ,tf$keras$metrics$TrueNegatives(name='tn')
        )
    )
}
```

```{r Iteration plot, fig.height=7, fig.width=7, eval=FALSE, include=FALSE}
modeling_divnn$history$metrics %>%
  .[c('root_loss','val_root_loss','root_roc','val_root_roc','lr')] %>%
  do.call(cbind,.) %>%
  as.data.frame() %>%
  mutate(iteration=seq(nrow(.))) %>%
  gather(metric,value,-iteration) %>%
  mutate(set=str_remove_all(metric,'_loss|_roc')) %>%
  mutate(
    metric=case_when(
      metric=='lr'~'1. LR'
      ,str_detect(metric,'loss')~'2. Loss'
      ,str_detect(metric,'roc')~'3. AUROC'
      ,TRUE~''
    )
  ) %>%
  qplot(iteration,value,color=set,data=.,geom='line') +
  geom_smooth(method='loess',formula=y~x,se=F) +
  facet_wrap(~metric,scales='free_y',ncol=1) +
  theme_minimal()
```

```{r Conduct hyperparameter tuning for a DI-VNN timing regressor, include=FALSE}
lambda_reg=10^seq(-10,-1,len=10)
if(run_heavy_computation){
  cat('Conduct hyperparameter tuning for DI-VNN timing regressor model\n')
  cat('Started:',as.character(now()),'\n')
    
    surrogate_model_reg=
      regressor_generator(
        output_reg
        ,epochs=5
        ,patience=round(5/2)
        ,batch_size=512
        ,warm_up=0.05
        ,lr=2^-6
        ,min_lr=2^-6/512
        ,tuning_mode=T
        ,verbose=1
      )
    
    tuning_divnn_reg=list()
    i=1
    for(j in seq(i,length(lambda_reg))){
      suppressWarnings(set.seed(33,sample.kind=sample.kind))
      tuning_divnn_reg[[j]]=surrogate_model_reg(lambda_reg[j])
    }
  
  rm(i,j)
  cat('End:',as.character(now()))
  saveRDS(tuning_divnn_reg,'data/tuning_divnn_reg.rds')
}else{
  cat(readRDS('data/log.rds')[['tuning_divnn_reg']])
  tuning_divnn_reg=readRDS('data/tuning_divnn_reg.rds')
}
```

```{r DI-VNN timing regression tuning results, eval=FALSE, include=FALSE}
tuning_divnn_reg %>%
  sapply(function(x)min(x$Score)) %>%
  data.frame(mean_RMSE=.) %>%
  mutate(which_param=seq(nrow(.))) %>%
  mutate(best=which.min(mean_RMSE)) %>%
  qplot(which_param,mean_RMSE,color=which_param==best,data=.) +
  geom_smooth(method='loess',color='black',formula=y~x) +
  scale_x_continuous(breaks=seq(lambda_reg))
```

```{r Conduct modeling for a DI-VNN timing regressor, include=FALSE}
if(run_heavy_computation){
  cat('Conduct modeling for DI-VNN timing regressor\n')
  cat('Started:',as.character(now()),'\n')
    
    surrogate_model_reg=
      regressor_generator(
        output_reg
        ,path='data/ontonet_reg'
        ,epochs=500
        ,patience=100
        ,batch_size=512
        ,warm_up=0.05
        ,lr=2^-6
        ,min_lr=2^-6/512
        ,tuning_mode=F
        ,checkpoint=T
        ,verbose=1
      )
    suppressWarnings(set.seed(33,sample.kind=sample.kind))
    modeling_divnn_reg=surrogate_model_reg(lambda_reg[3])
    
  cat('End:',as.character(now()))
  save_model_weights_hdf5(modeling_divnn_reg$ontonet,'data/ontonet_reg.h5')
  saveRDS(modeling_divnn_reg,'data/modeling_divnn_reg.rds')
}else{
  cat(readRDS('data/log.rds')[['modeling_divnn_reg']])
  modeling_divnn_reg=readRDS('data/modeling_divnn_reg.rds')
  refresh_session()
  modeling_divnn_reg$ontonet=
    output_reg %>%
    generator.ontonet(
      l2_norm=lambda_reg[3]
      ,output_unit=1
      ,output_activation=NULL
    ) %>%
    # readLines('data/ontonet.json') %>%
    # model_from_json() %>%
    load_model_weights_hdf5('data/ontonet_reg.h5') %>%
    compile(
      optimizer=optimizer_sgd(
        lr=modeling_divnn_reg$history$metrics$lr %>%
          .[which.min(modeling_divnn_reg$history$metrics$val_root_rmse)]
        ,momentum=0.9
      )
      ,loss='mean_squared_error'
      ,loss_weights=c(rep(
          0.3/(0.3*(length(.$outputs)-1)+1),length(.$outputs)-1)
          ,1/(0.3*(length(.$outputs)-1)+1)
        )
      ,metrics=c(tf$keras$metrics$RootMeanSquaredError(name='rmse'))
    )
}
```

```{r Reg iteration plot, fig.height=7, fig.width=7, eval=FALSE, include=FALSE}
modeling_divnn_reg$history$metrics %>%
  .[c('root_loss','val_root_loss','root_rmse','val_root_rmse','lr')] %>%
  do.call(cbind,.) %>%
  as.data.frame() %>%
  mutate(iteration=seq(nrow(.))) %>%
  gather(metric,value,-iteration) %>%
  mutate(set=str_remove_all(metric,'_loss|_rmse')) %>%
  mutate(
    metric=case_when(
      metric=='lr'~'1. LR'
      ,str_detect(metric,'loss')~'2. Loss'
      ,str_detect(metric,'rmse')~'3. RMSE'
      ,TRUE~''
    )
  ) %>%
  qplot(iteration,value,color=set,data=.,geom='line') +
  geom_smooth(method='loess',formula=y~x,se=F) +
  facet_wrap(~metric,scales='free_y',ncol=1) +
  theme_minimal()
```

```{r Create a function to transform test data for DI-VNN input, include=FALSE}
source('R/test_transformer-function.R')
```

```{r Transform test data for DI-VNN input, include=FALSE}
if(run_heavy_computation){
  test_data=
    list('calib','ran','geo','tem','bgt') %>%
    lapply(function(x){
      cat('Compile data for set of:',x,'\n')
      
      # Select the subset.
      if(x=='int'){
        data=
          pw_hlin %>%
          .[,!pData(phenoData(.))$censoring] %>%
          .[,pData(protocolData(.))$int]
      }else if(x=='calib'){
        data=
          pw_hlin %>%
          .[,!pData(phenoData(.))$censoring] %>%
          .[,pData(protocolData(.))$int] %>%
          .[,!colnames(.)%in%training_parameters$pre_calib_set,drop=F]
      }else{
        data=
          pw_hlin %>%
          .[,!pData(phenoData(.))$censoring] %>%
          .[,pData(protocolData(.))[[x]]]
      }
      
      # Transform the subset.
      test_transformer(
        test_data=data
        ,SGD1bit_fit=input$fit
        ,similarity=input$similarity
        ,mapping=input$mapping
        ,ontology=input$ontology
        ,ranked=T
        ,dims=7
        ,decreasing=F
        ,seed_num=33
      )
    }) %>%
    setNames(c('calib','ran','geo','tem','bgt'))
  saveRDS(test_data,'data/test_data.rds')
}else{
  cat(readRDS('data/log.rds')[['test_data']])
  test_data=readRDS('data/test_data.rds')
}
```

```{r Change outcome of test data for timing regression, include=FALSE}
if(run_heavy_computation){
  test_data_reg=
    list('int','calib','ran','geo','tem','bgt') %>%
    
    # Select the subset.
    lapply(function(x){
      cat('Compile data for set of:',x,'\n')
      if(x=='int'){
        data=
          pw_hlin %>%
          .[,!pData(phenoData(.))$censoring] %>%
          .[,pData(protocolData(.))$int]
      }else if(x=='calib'){
        data=
          pw_hlin %>%
          .[,!pData(phenoData(.))$censoring] %>%
          .[,pData(protocolData(.))$int] %>%
          .[,!colnames(.)%in%training_parameters$pre_calib_set,drop=F]
      }else{
        data=
          pw_hlin %>%
          .[,!pData(phenoData(.))$censoring] %>%
          .[,pData(protocolData(.))[[x]]]
      }
      
      # Transform the subset.
      data2=
        data %>%
        test_transformer(
          SGD1bit_fit=input$fit
          ,similarity=input$similarity
          ,mapping=input$mapping
          ,ontology=input$ontology
          ,ranked=T
          ,dims=7
          ,decreasing=F
          ,seed_num=33
        )
      
      # Change outcome with the time of delivery.
      data2 %>%
        `phenoData<-`(
          data2 %>%
            phenoData() %>%
            `pData<-`(
              data2 %>%
                pData() %>%
                rownames_to_column(var='id') %>%
                mutate(
                  outcome=
                    data %>%
                    protocolData() %>%
                    pData() %>%
                    rownames_to_column(var='id') %>%
                    mutate(
                      timing=as.duration(latest_date-admission_date)/ddays(1)
                    ) %>%
                    select(id,timing) %>%
                    pull(timing) %>%
                    setNames(colnames(data))
                ) %>%
                column_to_rownames(var='id')
            )
        )
      
    }) %>%
    setNames(c('int','calib','ran','geo','tem','bgt'))
  saveRDS(test_data_reg,'data/test_data_reg.rds')
}else{
  cat(readRDS('data/log.rds')[['test_data_reg']])
  test_data_reg=readRDS('data/test_data_reg.rds')
}
```

```{r Build a function for DI-VNN evaluation, include=FALSE}
source('R/eval_divnn-function.R')
```

```{r Build a function for DI-VNN calibration and evaluation, include=FALSE}
source('R/divnn_calibrator_evaluator-function.R')
```

```{r Build a function for DI-VNN timing regression evaluation, include=FALSE}
source('R/eval_divnn_reg-function.R')
```

```{r Build a function for DI-VNN time-regression and evaluation, include=FALSE}
source('R/divnn_timereg_evaluator-function.R')
```

## Step 21

The DI-VNN calibration may be considered, as described in the main text. The 
calibration used a ~20% split of a training set. The calibration set may be 
used to compare the predictive performance with other models, if any.

```{r Calibrate and evaluate DI-VNN, include=FALSE}
if(run_heavy_computation){
  c(model$divnn
    ,calib_model$divnn
    ,eval_model$divnn) %<-% divnn_calibrator_evaluator(
      data=test_data
      ,modeling=modeling_divnn
      ,training=training_parameters
      ,batch_size=512
      ,title='Calibrate and evaluate DI-VNN'
      ,dir='data'
      ,file='divnn'
    )
}else{
  cat(readRDS('data/log.rds')[['divnn']])
  c(model$divnn
    ,calib_model$divnn
    ,eval_model$divnn) %<-% list(
      readRDS('data/divnn.rds')
      ,readRDS('data/calib_divnn.rds')
      ,readRDS('data/eval_divnn.rds')
    )
}
```

```{r Evaluate DI-VNN regressor, include=FALSE}
if(run_heavy_computation){
  c(timing_model$divnn
    ,eval_timing_model$divnn) %<-% divnn_timereg_evaluator(
      timing_data=test_data_reg
      ,data=test_data
      ,timing_model=modeling_divnn_reg
      ,model=modeling_divnn
      ,calib_model=calib_model$divnn
      ,batch_size=512
      ,title='Evaluate DI-VNN regressor'
      ,dir='data'
      ,file='divnn'
    )
}else{
  cat(readRDS('data/log.rds')[['timing_divnn']])
  c(timing_model$divnn
    ,eval_timing_model$divnn) %<-% list(
      readRDS('data/timing_divnn.rds')
      ,readRDS('data/eval_timing_divnn.rds')
    )
}
```

## Step 22

Population-level data exploration is briefly described in the main text. We can 
provide interactive figure and table of DI-VNN. We used all pre-calibration set 
for population-level exploration. For simplicity, we only demonstrated 
population-level exploration of the classification model. 

```{r Visualization tables, include=FALSE}
if(run_heavy_computation){
  visualization=list()
  
  cat('Get ontonet visualisation table.\n')
  visualization$ontonet=
    output %>%
    viz.ontonet(
      feature=F
      ,eval.results=modeling_divnn$evaluation
      ,eval.metric='roc'
      ,eval.pal=c('#E64B35FF','#00A087FF')
    )
  
  cat('Get ontoarray visualisation table.\n')
  visualization$ontoarray=
    output %>%
    viz.ontoarray(modeling_divnn$ontonet,batch_size=512,verbose=T)
  
  saveRDS(visualization,'data/visualization.rds')
}else{
  cat(readRDS('data/log.rds')[['visualization']])
  visualization=readRDS('data/visualization.rds')
}
```

```{r Build a function to plot ontonet, include=FALSE}
source('R/plot.viz.ontonet-function.R')
```

```{r Ontonet, fig.height=12, fig.width=18, eval=FALSE, include=FALSE}
visualization$ontonet %>%
  plot.viz.ontonet(
    node.size=8
    ,node.shape='square'
    ,label=T
    ,label.family='serif'
    ,label.cex=0.85
    ,label.color='black'
    ,asp=0
    ,ylim=c(-1,1)
    ,xlim=c(-1,1)
  )
```

```{r Build a function to plot ontoarray, include=FALSE}
source('R/plot.viz.ontoarray-function.R')
```

```{r Ontoarray, fig.height=42, fig.width=12, eval=FALSE, include=FALSE}
visualization$ontoarray %>%
  plot.viz.ontoarray(
    pal=c('#E64B35FF','#00A087FF')
    ,label=T
    ,grid_col=12
  )
```

```{r Filter non-zero ontotype, echo=FALSE}
visualized_codes=
  
  # Get feature data from the ontoarray.
  visualization$ontoarray %>%
  lapply(function(x){
    x$ontotype %>%
      lapply(X=seq(nrow(.)),Y=.,Z=x$output,function(X,Y,Z){
        K=Z[Y$x[X],Y$y[X],Y$z[X]]
        if(K==0){
          NULL
        }else{
          K
        }
      }) %>%
      setNames(x$ontotype$feature)
  }) %>%
  setNames(names(visualization$ontoarray)) %>%
  unlist() %>%
  data.frame(output=.) %>%
  rownames_to_column(var='ontotype') %>%
  separate(ontotype,c('ontotype','code'),sep='\\.') %>%
  
  # Add tendency to event or nonevent.
  mutate(outcome=ifelse(output>=0,'event','nonevent')) %>%
  
  # Add the description.
  left_join(
    pw_hlin %>%
      fData() %>%
      rownames_to_column(var='code')
    ,by='code'
  ) %>%
  select(outcome,ontotype,code,desc) %>%
  arrange(ontotype,code,outcome) %>%
  
  # Simplify outcome.
  group_by(code,ontotype,desc) %>%
  summarize(outcome=paste0(outcome,collapse=' & '),.groups='drop') %>%
  mutate(outcome=ifelse(outcome=='event & nonevent','both',outcome)) %>%
  arrange(code,ontotype,factor(outcome,c('event','nonevent','both')))
```

```{r Show non-zero ontotype, eval=FALSE, include=FALSE}
visualized_codes %>%
  kable() %>%
  kable_classic()
```

```{r Prepare report of DI-VNN exploration, include=FALSE}
# Create an empty list to save DI-VNN exploration.
divnn_plot=list()

# Construct ggnetwork table the ontology network.
divnn_plot$base_data=
  visualization$ontonet$edge %>%
  graph_from_data_frame(directed=T) %>%
  ggnetwork(layout=layout_as_tree(.,mode='in')) %>%
  left_join(rename(visualization$ontonet$node,name=node),by='name')

# Plot the ontology network.
divnn_plot$base=
  divnn_plot$base_data %>%
  ggplot(aes(x=x,y=y,xend=xend,yend=yend,fill=avg)) +
  geom_edges(
    arrow=arrow(length=unit(5,'pt'),type='closed')
    ,show.legend=F
  ) +
  geom_nodes(
    aes(
      x=ifelse(avg>=0.58,NA,x)
      ,color=avg
    )
    ,show.legend=F
    ,size=8
    ,na.rm=T
  ) +
  geom_nodelabel(
    aes(label=ifelse(lb>0.55,paste0(formatC(round(avg,3),decimal.mark='.')),NA))
    ,family='sans'
    ,size=unit(4,'pt')
    ,alpha=0.95
    ,show.legend=F
    ,na.rm=T
  ) +
  geom_label(
    aes(
      label=
        ifelse(
          name%in%paste0('ONT:',c(154,171,144,155,149,167))
          ,str_remove_all(name,'ONT:')
          ,NA
        )
    )
    ,fill='white'
    ,family='sans'
    ,size=unit(3,'pt')
    ,segment.size=1
    ,alpha=0.2
    ,hjust=0.5
    ,vjust=0.5
    ,nudge_x=0
    ,nudge_y=0
    ,show.legend=F
    ,na.rm=T
  ) +
  scale_color_gradient(low='#E64B35FF',high='#00A087FF') +
  scale_fill_gradient(low='#E64B35FF',high='#00A087FF') +
  theme_blank()

# Convert all ontology arrays into a dataframe.
ontoarrays=
  visualization$ontoarray %>%
  lapply(X=names(.),Y=.,function(X,Y){
    ontoarray = Y[[X]]$output
    ontoarray %>%
      lapply(X=seq(dim(.)[3]),Y=.,Z=X,FUN=function(X,Y,Z){
        Y[, , X] %>% matrix()
      }) %>%
      do.call(rbind, .) %>%
      as.data.frame() %>% 
      setNames("fill") %>%
      mutate(
        x=rep(1:dim(ontoarray)[1],dim(ontoarray)[2]* dim(ontoarray)[3])
        ,y=
          rep(1:dim(ontoarray)[2],dim(ontoarray)[1]) %>%
          sort() %>%
          rep(dim(ontoarray)[3])
        ,z=
          rep(1:dim(ontoarray)[3], dim(ontoarray)[1]*dim(ontoarray)[2]) %>%
          sort()
      ) %>%
      left_join(
        Y[[X]]$ontotype
        ,by=c("x","y","z")
      ) %>% 
      mutate(ontology=X)
  }) %>%
  do.call(rbind,.) %>%
  mutate(z=paste0("z=",z))

source('R/ontoarray-function.R')
source('R/ontoarray_tab-function.R')

# Get data for the ontoarrays of interest.
ontogroup=
  visualized_codes %>%
  filter(code%in%c(
    'N760'
    ,'B379'
    ,'causal_A03'
    ,'598'
    ,'8602'
    ,'H521'
    ,'H522'
    ,'H527'
    ,'9059'
    ,'734'
  )) %>%
  mutate(
    desc=sapply(X=seq(nrow(.)),Y=code,Z=desc,function(X,Y,Z){
      ifelse(
        str_detect(Y[X],'causal_')
        ,baseline_nodes$name[
          baseline_nodes$label==str_remove_all(Y[X],'causal_')
        ]
        ,Z[X]
      )
    })
  ) %>%
  filter(
    ontotype%in%paste0('ONT:',c(171,154,144,149,155,167))
    | code%in%c('8602','causal_A03')
  ) %>%
  group_by(code) %>%
  filter(seq(n())==1) %>%
  ungroup() %>%
  mutate(
    ontotype=ifelse(code%in%c(8602,'causal_A03'),'',ontotype)
    ,outcome=ifelse(code%in%c(8602,'causal_A03'),'',outcome)
  ) %>%
  mutate(text=paste(code,desc)) %>%
  mutate(text=str_wrap(text,width=40)) %>%
  arrange(
    outcome
    ,factor(
      code
      ,c(
        'N760'
        ,'B379'
        ,'causal_A03'
        ,'598'
        ,'8602'
        ,'H521'
        ,'H522'
        ,'H527'
        ,'9059'
        ,'734'
      )
    )
  ) %>%
  pull(text)

# Plot the ontology arrays on the top of the network.
divnn_plot$top=
  divnn_plot$base +
  annotation_custom(grob=ontoarray(171,c(2),1),0.85,1.05,0.475,0.875) +
  annotation_custom(grob=ontoarray(171,c(9),1),0.9,1,0.35,0.55) +
  annotation_custom(grob=ontoarray(154,c(2),1),0.800,0.935,-0.100,0.135) +
  annotation_custom(grob=ontoarray(144,c(3),1),0.59,0.69,0.05,0.25) +
  annotation_custom(grob=ontoarray(149,c(3),1),0.45,0.55,0.2,0.4) +
  annotation_custom(grob=ontoarray(155,c(2),1,legend=T),0.07,0.17,0.39,0.69) +
  annotation_custom(grob=ontoarray(167,c(3,6,7),2),0.59,0.79,0.5,0.9) +
  annotate(
    geom='text'
    ,label=
      ontogroup[8:10] %>%
      paste0(collapse='\n')
    ,x=0,y=0.3,hjust=0,vjust=0,size=unit(3,'pt'),color='#E64B35FF'
    ,family='sans'
  ) +
  annotate(
    geom='text'
    ,label=
      ontogroup[1:2] %>%
      paste0(collapse='\n')
    ,x=0,y=0.2,hjust=0,vjust=0,size=unit(3,'pt'),alpha=0.75
    ,family='sans'
  ) +
  annotate(
    geom='text'
    ,label=
      ontogroup[3:7] %>%
      paste0(collapse='\n')
    ,x=0,y=0,hjust=0,vjust=0,size=unit(3,'pt'),color='#00A087FF'
    ,family='sans'
  )
```

```{r Save data population-level exploration, eval=FALSE, include=FALSE}
divnn_plot$base_data %>%
  select(name,avg,lb,ub) %>%
  setNames(
    c('Ontology'
      ,'AUROC 95% CI'
      ,'Lower bound'
      ,'Upper bound')
  ) %>%
  
  kable() %>%
  kable_classic()

rbind(
    ontoarray_tab(171,2)
    ,ontoarray_tab(171,9)
    ,ontoarray_tab(144,3)
    ,ontoarray_tab(154,2)
    ,ontoarray_tab(155,2)
    ,ontoarray_tab(167,3)
    ,ontoarray_tab(167,6)
    ,ontoarray_tab(167,7)
    ,ontoarray_tab(149,3)
  ) %>%
  setNames(
    c('Ontology'
      ,'Channel'
      ,colnames(.)[-1:-2])
  ) %>%
  
  kable() %>%
  kable_classic()
```

```{r Save image of population-level exploration, eval=FALSE, include=FALSE}
# fig.height=8.26772, fig.width=7.08661 on top of this chunk if shown here
divnn_plot$top
```

```{r Show complete non-zero ontotype, eval=FALSE, include=FALSE}
visualized_codes %>%
  kable() %>%
  kable_classic()
```

```{r A function to visualize ontoarray of estimation DI-VNN, include=FALSE}
source('R/viz.ontoarray.reg-function.R')
```

```{r Get ontoarray visualisation of timing regression table., include=FALSE}
if(run_heavy_computation){
  cat('Get ontoarray visualisation of timing regression table.\n')
  visualization$ontoarray_reg=
    output_reg %>%
    viz.ontoarray.reg(modeling_divnn_reg$ontonet,batch_size=512,verbose=T)
  
  saveRDS(visualization$ontoarray_reg,'data/visualization_ontoarray_reg.rds')
}else{
  cat(readRDS('data/log.rds')[['visualization_ontoarray_reg']])
  visualization$ontoarray_reg=readRDS('data/visualization_ontoarray_reg.rds')
}
```

```{r Save weights of DI-VNN, include=FALSE}
model_weight=
  # Get the classification DI-VNN weights of the representation layers.
  visualization$ontoarray %>%
  lapply(function(x){
    x$ontotype %>%
      lapply(X=seq(nrow(.)),Y=.,Z=x$output,function(X,Y,Z){
        K=Z[Y$x[X],Y$y[X],Y$z[X]]
        if(K==0){
          NULL
        }else{
          K
        }
      }) %>%
      setNames(x$ontotype$feature)
  }) %>%
  setNames(names(visualization$ontoarray)) %>%
  unlist() %>%
  data.frame(output=.) %>%
  rownames_to_column(var='ontology') %>%
  rename(classification_output=output) %>%
  
  # Get the estimation DI-VNN weights of the representation layers.
  left_join(
    visualization$ontoarray_reg %>%
      lapply(function(x){
        x$ontotype %>%
          lapply(X=seq(nrow(.)),Y=.,Z=x$output,function(X,Y,Z){
            K=Z[Y$x[X],Y$y[X],Y$z[X]]
            if(K==0){
              NULL
            }else{
              K
            }
          }) %>%
          setNames(x$ontotype$feature)
      }) %>%
      setNames(names(visualization$ontoarray)) %>%
      unlist() %>%
      data.frame(output=.) %>%
      rownames_to_column(var='ontology') %>%
      rename(estimation_output=output)
    ,by='ontology'
  ) %>%
  separate(ontology,c('ontology','predictor'),sep='\\.') %>%
  
  # Get the predictor annotation.
  left_join(
    rbind(
        annotation %>%
          rename(predictor=code,description=desc)
        ,baseline_nodes %>%
          mutate(label=paste0('causal_',label)) %>%
          rename(predictor=label,description=name)
      )
    ,by='predictor'
  ) %>%
  
  # Get the predictor position.
  left_join(
    output %>%
      fData() %>%
      rownames_to_column(var='xyz') %>%
      rename(predictor=feature) %>%
      select(xyz,predictor) %>%
      filter(!is.na(predictor)) %>%
      mutate(xyz=str_remove_all(xyz,'x')) %>%
      separate(xyz,c('dimension_1','yz'),sep='y') %>%
      separate(yz,c('dimension_2','dimension_3'),sep='z')
    ,by='predictor'
  ) %>%
  
  # Wrap up.
  arrange(ontology,dimension_3,dimension_2,dimension_1,predictor) %>%
  select(
    ontology
    ,dimension_1
    ,dimension_2
    ,dimension_3
    ,predictor
    ,description
    ,everything()
  ) %>%
  setNames(str_to_sentence(str_replace_all(colnames(.),'_',' ')))
```

```{r Show the weights, eval=FALSE, include=FALSE}
model_weight %>%
  kable() %>%
  kable_classic()
```

```{r Save ontology table, eval=FALSE, include=FALSE}
input$ontology %>%
  filter(relation!='feature') %>%
  select(-relation) %>%
  setNames(str_to_sentence(colnames(.))) %>%
  
  kable() %>%
  kable_classic()
```

## Step 23

For individual-level exploration, we prepared an example dataset. Intermediate 
layers are extracted from this example. For simplicity, we only demonstrated 
individual-level exploration of the classification model. Description for this 
exploration is already clearly described in the main text.

```{r Save variables for app, eval=FALSE, include=FALSE}
# Save the feature and the reg expression.
int_nps_eol_p %>%
  select(key) %>%
  mutate_at('key',function(x){
    ifelse(str_detect(x,'causal_'),paste0(str_remove_all(x,'causal_'),'*'),x)
  }) %>%
  left_join(rename(measure_nodes,key=label),by='key') %>%
  mutate(name=ifelse(is.na(name),key,name)) %>%
  mutate_at('key',function(x){
    ifelse(str_detect(x,'\\*'),paste0('causal_',str_remove_all(x,'\\*')),x)
  }) %>%
  setNames(c('features','regex')) %>%
  saveRDS('data/features.rds')

# Save nationwide historical rate.
preproc(nw_int_hlin)$hist_rate %>%
  filter(mh%in%readRDS('data/features.rds')$features) %>%
  saveRDS('data/hist_rates.rds')

# Reduce caret object like random forest.
source('R/reduce_rf-function.R')
readRDS('data/calib_divnn.rds') %>%
  reduce_rf() %>%
  saveRDS('data/small_calib_divnn.rds')

# Save the ontonotation.
model_weight %>%
  select(Ontology,Predictor,Description) %>%
  filter(!duplicated(.)) %>%
  arrange(desc(Ontology),Predictor,Description) %>%
  saveRDS('data/ontonotation.rds')
```

```{r Prepare ROC and prediction tables, eval=FALSE, include=FALSE}
# ROC table
calib_model$divnn$pred %>%
  
  # Take needed columns.
  select(Resample,rowIndex,event,obs) %>%
  setNames(c('subset','index','score','outcome')) %>%
  mutate(outcome=as.double(outcome=='event')) %>%
  
  # Create multiple thresholds.
  left_join(
    select(.,index) %>%
      filter(!duplicated(.)) %>%
      arrange(index) %>%
      cbind(
        data.frame(
            key=paste0('th_',str_pad(0:100,3,'left','0'))
            ,value=seq(0,1,len=101)
          ) %>%
          spread(key,value)
      )
    ,by='index'
  ) %>%
  
  # Compute confusion matrix.
  gather(key,th,-subset,-index,-score,-outcome) %>%
  select(-key) %>%
  mutate(pred=as.integer(score>th)) %>%
  mutate(
    tp=as.integer(outcome==1 & pred==1)
    ,fn=as.integer(outcome==1 & pred==0)
    ,fp=as.integer(outcome==0 & pred==1)
    ,tn=as.integer(outcome==0 & pred==0)
  ) %>%
  select(-index,-score,-outcome,-pred) %>%
  group_by(subset,th) %>%
  summarize_all(sum) %>%
  ungroup() %>%
  
  # Compute evaluation metrics.
  mutate(
    tpr=tp/(tp+fn+1e-17)
    ,tnr=tn/(tn+fp+1e-17)
    ,ppv=tp/(tp+fp+1e-17)
    ,npv=tn/(tn+fn+1e-17)
  ) %>%
  select(-subset,-tp,-fn,-fp,-tn) %>%
  
  # Compute ROC components.
  group_by(th) %>%
  summarize_all(function(x){
    y=mean(x)+(-1:1)*qnorm(0.975)*sd(x)/sqrt(length(x))
    paste0(y,collapse='|')
  }) %>%
  gather(metric,interval,-th) %>%
  separate(interval,c('lb','avg','ub'),sep='\\|') %>%
  mutate_at(c('lb','avg','ub'),as.numeric) %>%
  mutate_at(c('lb','avg','ub'),function(x){
    round(ifelse(x<0,0,ifelse(x>1,1,x)),4)
  }) %>%
  
  # Save the ROC table.
  saveRDS('data/classification.rds')

# Prediction table
modeling_divnn$ontonet %>%
  
  # Use calibration split for classification.
  predict_generator(
    generator=
      generator.ontoarray(
        test_data$calib
        ,seq(ncol(test_data$calib))
        ,batch_size=512
      )
    ,steps=ceiling(ncol(test_data$calib)/512)
    ,verbose=1
  ) %>%
  setNames(
    modeling_divnn$history$metrics %>%
      names() %>%
      .[str_detect(.,'val_')
        &str_detect(.,'_loss')
        &str_detect(.,'ONT|root')] %>%
      str_remove_all('val_|_loss')
  ) %>%
  lapply(function(y){
    y[seq(ncol(test_data$calib)),,drop=F]
  }) %>%
  c(list(outcome=test_data$calib$outcome)) %>%
  as.data.frame() %>%
  select(root,outcome) %>%
  rename(event=root) %>%
  mutate(nonevent=event) %>%
  select(nonevent,everything()) %>%
  `rownames<-`(colnames(test_data$calib)) %>%
  
  # Get the calibrated, predicted probability.
  cbind(
    suppressWarnings(predict(calib_model$divnn,newdata=.,type='prob')) %>%
      rename_all(function(x)paste0(x,'2'))
  ) %>%
  select(-nonevent,-event) %>%
  rename_all(function(x)str_remove_all(x,'2')) %>%
  select(event) %>%
  
  # Save the prediction table.
  saveRDS('data/prediction.rds')
```

```{r Show training samples with the best predictions, eval=FALSE, include=FALSE}
pw_hlin %>%
  
  # Select true classification in estimation data of calibration split.
  .[,eval_timing_model$divnn$timing_calib %>%
      rownames_to_column(var='id') %>%
      filter(as.integer(calib_pred=='event')==outcome) %>%
      arrange(timing_pred) %>%
      pull(id)
  ] %>%
  
  # Get the phenotype and protocol data.
  lapply(X=1,Y=.,function(X,Y){
    Z=phenoData(Y) %>%
      pData()
    K=protocolData(Y) %>%
      pData()
    Z %>%
      rownames_to_column(var='id') %>%
      left_join(
        K %>%
          rownames_to_column(var='id')
        ,by='id'
      )
  }) %>%
  .[[1]] %>%
  select(-outcome) %>%
  
  # Join the estimation data of calibration split with true classification 
  # to the previous data.
  left_join(
    eval_timing_model$divnn$timing_calib %>%
      rownames_to_column(var='id') %>%
      filter(as.integer(calib_pred=='event')==outcome) %>%
      arrange(timing_pred)
    ,by='id'
  ) %>%
  
  # Convert the estimation results as week.
  mutate(timing_pred=round(timing_pred/7),timing=round(timing/7)) %>%
  
  # Get the estimation results that have error less than 6 weeks 
  # for predicted time of delivery between 6 and 26 weeks' gestation.
  select(
    subject_id
    ,healthcare_id
    ,admission_date
    ,outcome
    ,pred
    ,calib_pred
    ,timing
    ,timing_pred
    ,everything()
  ) %>%
  
  # Save these training examples with the best predictions.
  saveRDS('data/subject_examples.rds')
```

```{r Create individual ontoarray extractor function, include=FALSE}
source('R/ontoarray1-function.R')
```

```{r Show input example for the app, eval=FALSE, include=FALSE}
readRDS('data/subject_examples.rds') %>%
  filter(subject_id=='51160204.1' & healthcare_id=='051103')
```

```{r Input example for the app, include=FALSE}
app=list()

app$subject_id='51160204.1'
app$healthcare_id='051103'

app$raw_visit=
  readRDS('data/target_visits.rds') %>%
  filter(subject_id==app$subject_id & healthcare_id==app$healthcare_id) %>%
  select(admission_date,code)

app$prediction_date=as.Date('2016-07-30')
```

```{r Show input table, eval=FALSE, include=FALSE}
app$raw_visit %>%
  filter(admission_date<=app$prediction_date) %>%
  kable() %>%
  kable_classic()
```

```{r Show the true outcome and the timing, eval=FALSE, include=FALSE}
c(outcome=
    readRDS('data/outcome.rds') %>%
    filter(subject_id==app$subject_id) %>%
    mutate(outcome=ifelse(is.na(outcome),0,as.integer(outcome=='event'))) %>%
    pull(outcome)
  ,timing=
    readRDS('data/outcome.rds') %>%
    filter(subject_id==app$subject_id) %>%
    mutate(timing=as.duration(latest_date-app$prediction_date)/ddays(1)) %>%
    mutate(timing=round(timing/7)) %>%
    pull(timing)
  )
```

```{r Processing codes at the server side, include=FALSE}
if(run_heavy_computation){
  cat('Start',as.character(now()),'\n')
  
  # Create null input if no positive features are found.
  app$input0=
    readRDS('data/features.rds') %>%
    mutate(I1=NA) %>%
    select(-regex) %>%
    column_to_rownames(var='features')
  
  if(nrow(app$raw_visit)==0){
    # Use the null input if no visit.
    app$input=app$input0
  }else{
    # Filter visit up to the current date (the prediction date).
    app$visit_up_to_current=
      app$raw_visit %>%
      filter(admission_date<=app$prediction_date)
    
    if(nrow(app$visit_up_to_current)==0){
      # Use the null input if no visit up to the current date.
      app$input=app$input0
    }else{
      # Only use the latest encounter for each code.
      app$visit_latest_enc=
        app$visit_up_to_current %>%
        group_by(code) %>%
        filter(admission_date==max(admission_date)) %>%
        ungroup()
      
      # Only use the codes for the features.
      app$visit_with_features=
        app$visit_latest_enc %>%
        filter(
          code
          %in%(
            readRDS('data/features.rds')$features %>%
              .[!str_detect(.,'causal_')]
          )
          |
          str_detect(
            code
            ,paste0(
              readRDS('data/features.rds')$regex %>%
                .[str_detect(readRDS('data/features.rds')$features,'causal_')]
              ,collapse='|'
            )
          )
        )
      
      if(nrow(app$visit_with_features)==0){
        # Use the null input if no positive features are found.
        app$input=app$input0
      }else{
        # Create the input dataframe with the positive features.
        app$input=
          readRDS('data/features.rds') %>%
          sapply(X=seq(nrow(.)),Y=.,Z=app$visit_with_features,function(X,Y,Z){
            if(str_detect(Y$features[X],'causal_')){
              K=Z %>%
                filter(str_detect(str_to_upper(code),Y$regex[X]))
            }else{
              K=Z %>%
                filter(code==Y$regex[X])
            }
            if(nrow(K)==0){
              NA
            }else{
              as.duration(app$prediction_date-max(K$admission_date))/ddays(1)
            }
          }) %>%
          setNames(readRDS('data/features.rds')$features) %>%
          data.frame(I1=.)
      }
    }
  }
  
  # Transform into historical rates.
  app$hlin=
    ExpressionSet(
      assayData=
        app$input %>%
        as.matrix()
      ,phenoData=
        data.frame(outcome=NA,row.names='I1') %>%
        AnnotatedDataFrame()
      ,featureData=
        readRDS('data/features.rds') %>%
        column_to_rownames(var='features') %>%
        AnnotatedDataFrame()
    ) %>%
    trans_hist_rate(
      hist_rate=readRDS('data/hist_rates.rds')
      ,interpolation='linear'
      ,verbose=T
    )
  
  # Compile a tidy set.
  app$tidyset=
    test_transformer(
      test_data=app$hlin
      ,SGD1bit_fit=readRDS('data/input.rds')$fit
      ,similarity=readRDS('data/input.rds')$similarity
      ,mapping=readRDS('data/input.rds')$mapping
      ,ontology=readRDS('data/input.rds')$ontology
      ,ranked=T
      ,dims=7
      ,decreasing=F
      ,seed_num=33
    ) %>%
    `colnames<-`('I1')
  
  # Save a single tidy set 
  # for reducing time of the compiling process.
  saveRDS(app$tidyset,'data/single_tidyset.rds')
  
  # Calling DI-VNN.
  cat('Calling DI-VNN\n')
  refresh_session()
  app$ontonet=
    readRDS('data/output.rds') %>%
    generator.ontonet(l2_norm=0.1) %>%
    load_model_weights_hdf5('data/ontonet.h5') %>%
    compile(
      optimizer=optimizer_sgd(
        lr=readRDS('data/modeling_divnn.rds')$history$metrics$lr %>%
          .[which.max(
            readRDS('data/modeling_divnn.rds')$history$metrics$val_root_roc
          )]
        ,momentum=0.9
      )
      ,loss='mean_squared_error'
      ,loss_weights=c(rep(
          0.3/(0.3*(length(.$outputs)-1)+1),length(.$outputs)-1)
          ,1/(0.3*(length(.$outputs)-1)+1)
        )
      ,metrics=c(
          tf$keras$metrics$AUC(name='roc')
          ,tf$keras$metrics$TruePositives(name='tp')
          ,tf$keras$metrics$FalseNegatives(name='fn')
          ,tf$keras$metrics$FalsePositives(name='fp')
          ,tf$keras$metrics$TrueNegatives(name='tn')
        )
    )
  
  # Save the ontonet architecture, weights, and optimizer.
  save_model_hdf5(app$ontonet,'data/entire_ontonet.h5')
  
  # Compute prediction.
  cat('Compute prediction\n')
  app$outputs=
    app$ontonet %>%
    predict_on_batch(
      generator.ontoarray(
        app$tidyset
        ,seq(ncol(app$tidyset))
        ,batch_size=1
      )()[[1]]
    ) %>%
    lapply(as.array) %>%
    setNames(
      readRDS('data/modeling_divnn.rds')$history$metrics %>%
        names() %>%
        .[str_detect(.,'val_')
          &str_detect(.,'_loss')
          &str_detect(.,'ONT|root')] %>%
        str_remove_all('val_|_loss')
    ) %>%
    as.data.frame()
  
  # Calibrate prediction.
  cat('Calibrate prediction\n')
  app$prediction=
    app$outputs %>%
    select(root) %>%
    rename(event=root) %>%
    mutate(nonevent=event) %>%
    select(nonevent,everything()) %>%
    `rownames<-`(colnames(app$tidyset)) %>%
    cbind(
      suppressWarnings(
          predict(readRDS('data/small_calib_divnn.rds'),newdata=.,type='prob')
        ) %>%
        rename_all(function(x)paste0(x,'2'))
    ) %>%
    select(-nonevent,-event) %>%
    rename_all(function(x)str_remove_all(x,'2')) %>%
    select(event) %>%
    `rownames<-`(colnames(app$tidyset)) %>%
    pull(event)
  
  # Retrieve internal properties.
  cat('Retrieve internal properties\n')
  app$ontoarray=
    app$tidyset %>%
    ontoarray1(app$ontonet,1)
  
  cat('End',as.character(now()),'\n')
  
  saveRDS(app,'data/app.rds')
}else{
  cat(readRDS('data/log.rds')[['app']])
  app=readRDS('data/app.rds')
}
```

```{r Post-processing codes at the server side, include=FALSE}
# Create an empty list to save the results.
results=list()

# Since the prediction is positive, 
# use this precise threshold (the best positive predictive value).
app$threshold=0.67

# Manually inspect the prediction output.
readRDS('data/prediction.rds') %>%
  mutate(event=as.integer(event>app$threshold))

# Create a dataframe for the result conclusion.
results$main=
  c(prediction=ifelse(app$prediction>app$threshold,'event','nonevent')) %>%
  data.frame(result=.)
```

```{r Show prediction results of the input example, eval=FALSE, include=FALSE}
results$main %>%
  kable() %>%
  kable_classic()
```

```{r Show the timetable, eval=FALSE, include=FALSE}
app$ontoarray$representation %>%
  app$ontoarray$ontotable() %>%
  filter(!is.na(feature)) %>%
  select(-max_fill,-z_lab) %>%
  rename(output=fill) %>%
  left_join(
    pw_hlin %>%
      fData() %>%
      rownames_to_column(var='feature') %>%
      rename(description=desc)
    ,by='feature'
  ) %>%
  inner_join(
    app$input %>%
      filter(!is.na(I1)) %>%
      rownames_to_column(var='feature') %>%
      mutate(admission_date=app$prediction_date-I1)
    ,by='feature'
  ) %>%
  select(admission_date,ontology,feature,description,output) %>%
  arrange(desc(admission_date),output) %>%
  kable() %>%
  kable_classic()
```

```{r Create individual ontology network and array, include=FALSE}
source('R/ontoproperties1-function.R')
```

```{r Ontology network and array for ONT169, eval=FALSE, include=FALSE}
ontoproperties1('169',pointer_stroke=2,pointer_size=2.5)
```

```{r Ontology network and array for root, eval=FALSE, include=FALSE}
ontoproperties1('root',pointer_stroke=2,pointer_size=2.5)
```

```{r Save identity and results of example, include=FALSE}
results$identity_results=
  data.frame(
    row=1:5
    ,column=rep(1,5)
    ,text=
      c(paste0('Name: ','____________________________',' Age: _________')
        ,paste0(
          'Input: '
          ,app$raw_visit %>%
            filter(admission_date<=app$prediction_date) %>%
            pull(admission_date) %>%
            unique() %>%
            length()
          ,' visits consisting '
          ,app$raw_visit %>%
            filter(admission_date<=app$prediction_date) %>%
            nrow()
          ,' code entries'
        )
        ,paste0(
          'Time of prediction: '
          ,app$prediction_date
        )
        ,paste0(
          'Predicted outcome: '
          ,ifelse(
            app$prediction>app$threshold
            ,'Prelabor rupture of membranes (PROM)'
            ,'NOT Prelabor rupture of membranes (PROM)'
          )
        )
        ,paste0(
          'Predicted probability: '
          ,formatC(round(app$prediction,3),decimal.mark='.')
          ,' (threshold='
          ,formatC(app$threshold,decimal.mark='.')
          ,')'
        )
      )
  )
```

```{r Save data of individual-level exploration, eval=FALSE, include=FALSE}
rbind(
  
    # Identity results
    results$identity_results %>%
      select(text) %>%
      slice(-1) %>%
      separate(text,c('information','description'),sep=': ')
    
    # Ontology array
    ,app$ontoarray$representation %>%
      app$ontoarray$ontotable() %>%
      filter(str_remove_all(ontology,'ONT:')=='root') %>%
      mutate(
        z_lab=str_remove_all(z_lab,'=')
        ,x=paste0('x',x)
        ,y=paste0('y',y)
        ,feature=ifelse(is.na(feature),'',feature)
        ,yellow_square=
          feature
          %in% (
            app$input %>%
              filter(!is.na(I1)) %>%
              rownames_to_column(var='feature') %>%
              inner_join(
                app$ontoarray$representation %>%
                  app$ontoarray$ontotable()
                ,by='feature'
              ) %>%
              pull(feature) %>%
              .[!duplicated(.)]
          )
        ,yellow_square=ifelse(yellow_square,'- Yellow Square','')
        ,fill=round(fill,3)
      ) %>%
      select(z_lab,x,y,feature,yellow_square,fill) %>%
      arrange(z_lab,desc(x),y) %>%
      unite(position,x,y,sep='.') %>%
      unite(feature,feature,yellow_square,sep='') %>%
      mutate(feature=ifelse(feature=='',feature,paste0('(',feature,')'))) %>%
      unite(description,position,fill,feature,sep=' ') %>%
      mutate(information=paste0('Ontology array ',z_lab)) %>%
      select(information,description)
  ) %>%
  setNames(str_to_sentence(colnames(.))) %>%
  
  kable() %>%
  kable_classic()
```

```{r Save image of individual-level exploration, eval=FALSE, include=FALSE}
# fig.height=8.26772, fig.width=7.08661 on top of this chunk if shown here
ggarrange(
    results$identity_results %>%
        ggplot(aes(column,row)) +
        geom_text(aes(label=text),hjust=0,vjust=1,size=3,family='sans') +
        geom_blank(data=data.frame(column=3,row=7)) +
        scale_y_reverse() +
        theme_blank() +
        theme(
          title=element_text(size=unit(8,'pt'),face='bold',family='sans')
        ) +
        ggtitle('Identity and results')
    ,ontoproperties1('root',widths=c(2,1),pointer_stroke=1.5,pointer_size=1.4)
    ,heights=c(1.7,6.56772)
    ,ncol=1,nrow=2
  )
```


























